{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ftplib\n",
    "import datetime\n",
    "import requests\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from rasterio.mask import mask\n",
    "from shapely.geometry import Point\n",
    "from rasterio.transform import from_origin\n",
    "from netCDF4 import Dataset, num2date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_nc_file = \"../data/row\"\n",
    "path_nc_row = \"../repository/pre-processing/row\"\n",
    "path_modified = \"../repository/pre-processing/result-row\"\n",
    "path_pch_tabular = \"../data/tabular/data_pch_balai.xlsx\"\n",
    "mask_pulau = \"../data/geojson/pulau.geojson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_host = os.getenv(\"HOST\")\n",
    "ftp_user = os.getenv(\"USER\")\n",
    "ftp_password = os.getenv(\"PASSWORD\")\n",
    "cycle = \"12\"\n",
    "\n",
    "def connect_ftp():\n",
    "    ftp = ftplib.FTP(ftp_host)\n",
    "    ftp.login(ftp_user, ftp_password)\n",
    "    ftp.cwd(\"/\")\n",
    "    return ftp\n",
    "\n",
    "def download_file_from_ftp(ftp, filename):\n",
    "    try:\n",
    "        file_list = ftp.nlst()\n",
    "        if filename in file_list:\n",
    "            local_file_path = os.path.join(path_nc_file, filename)\n",
    "            if not os.path.exists(local_file_path):\n",
    "                with open(local_file_path, \"wb\") as local_file:\n",
    "                    ftp.retrbinary(f\"RETR {filename}\", local_file.write)\n",
    "                print(f\"Download successfully {filename}\")\n",
    "            else:\n",
    "                print(f\"File {filename} is available\")\n",
    "            return local_file_path\n",
    "    except Exception:\n",
    "        print(\"File is corrupted, and there is nothing that can be done.\")\n",
    "        return None\n",
    "\n",
    "def download_latest_file_from_ftp(ftp):\n",
    "    file_list = ftp.nlst()\n",
    "    if file_list:\n",
    "        latest_file = sorted(file_list)[-1]\n",
    "        return download_file_from_ftp(ftp, latest_file)\n",
    "    return None\n",
    "\n",
    "today = datetime.date.today() - datetime.timedelta(days=1)\n",
    "filename = f\"ECMWF.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "print(\"Downloading:\", filename)\n",
    "\n",
    "ftp = connect_ftp()\n",
    "if ftp:\n",
    "    local_file_path = download_file_from_ftp(ftp, filename) or download_latest_file_from_ftp(ftp)\n",
    "    ftp.quit()\n",
    "else:\n",
    "    print(\"Cannot connect to FTP server\")\n",
    "\n",
    "if local_file_path is None:\n",
    "    print(\"File is currently unavailable for download.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baca data .nc\n",
    "data = Dataset(local_file_path)\n",
    "\n",
    "lat = data.variables['lat'][:]\n",
    "lon = data.variables['lon'][:]\n",
    "prec = data.variables['tp'][:,0,:,:]\n",
    "time = data.variables['time'][:]\n",
    "\n",
    "dates = num2date(time, data.variables['time'].units)\n",
    "print((dates[7]+datetime.timedelta(hours=7)).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "print((dates[15]+datetime.timedelta(hours=7)).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "\n",
    "xrain = prec\n",
    "print(np.array_equal(xrain,prec))\n",
    "\n",
    "#change rainfall data\n",
    "for time in range (len(dates)):\n",
    "    for lati in range(len(lat)) :\n",
    "        for loni in range (len(lon)) :\n",
    "            if (time<=0) :\n",
    "                if (xrain[time,lati,loni]<=0) :\n",
    "                    xrain[time,lati,loni] == 0\n",
    "            elif(time>0) :\n",
    "                if (xrain[time,lati,loni]<0) :\n",
    "                    xrain[time,lati,loni] = xrain[time-1,lati,loni]\n",
    "                if (xrain[time,lati,loni]-xrain[time-1,lati,loni]<0) :\n",
    "                    xrain[time,lati,loni] = xrain[time-1,lati,loni]\n",
    "\n",
    "# initiate rain before changing rainfall accumulate to interval rainfall\n",
    "hjn = np.empty((len(dates),len(lat),len(lon)))\n",
    "# change rainfall accumulate to interval rainfall\n",
    "hjn[0,:,:] = xrain[0,:,:]\n",
    "for i in range (1,len(dates)) :\n",
    "    hjn[i,:,:] = xrain[i,:,:]-xrain[i-1,:,:]\n",
    "\n",
    "# reshape 3d to 4d so it can be sabed into netcdf permanent dimension\n",
    "hjn2 = hjn.reshape(len(dates), 1, len(lat), len(lon))\n",
    "\n",
    "# rewrite to netcdf\n",
    "ds = xr.open_dataset(local_file_path)\n",
    "ds['tp'].values = hjn2\n",
    "ds = ds.assign_coords(time=(\"time\",ds['time'].values + np.timedelta64(7,'h')))\n",
    "\n",
    "# output rewrite to netcdf\n",
    "output_rewrite = f\"ECMWF_new.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "output_path = os.path.join(path_nc_row, output_rewrite)\n",
    "ds.to_netcdf(output_path)\n",
    "print (ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_name = f\"ECMWF_new_3d.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "\n",
    "# Gabungkan path dengan nama file\n",
    "file_path = os.path.join(path_modified, result_file_name)\n",
    "\n",
    "# Buat file NetCDF baru\n",
    "f = Dataset(file_path, 'w', format='NETCDF4')\n",
    "print(f\"File {result_file_name} berhasil dibuat\")\n",
    "print (data.variables)\n",
    "\n",
    "# define variables foe new netcdf4 file\n",
    "rain = hjn[:48,:,:]\n",
    "latitude = data.variables[\"lat\"][:]\n",
    "longitude = data.variables[\"lon\"][:]\n",
    "time_k = data.variables['time'][:48]\n",
    "\n",
    "tempgrp = f.createGroup('Rain_data')\n",
    "\n",
    "# Create dimension for netCDF4\n",
    "f.createDimension('lon', len(longitude))\n",
    "f.createDimension('lat', len(latitude))\n",
    "f.createDimension('time', len(time_k))\n",
    "\n",
    "# Create variables for netCDF4\n",
    "lon = f.createVariable('lon', 'f4', 'lon')\n",
    "lat = f.createVariable('lat', 'f4', 'lat')  \n",
    "rain = f.createVariable('rain', 'f4', ('time', 'lat', 'lon'))\n",
    "time = f.createVariable('time', 'i4', 'time')\n",
    "\n",
    "# define variables to be saved into netcdf4 file\n",
    "lon[:] = longitude[:] #The \"[:]\" at the end of the variable instance is necessary\n",
    "lat[:] = latitude[:]\n",
    "rain[:,:,:] = hjn[:48,:,:]\n",
    "time[:] = time_k+7\n",
    "\n",
    "print (dates[0].strftime('%Y-%m-%d ')+str(cycle)+\":00:00\")\n",
    "\n",
    "#Add global attributes\n",
    "f.description = \"ECMWF from BMKG modified by Jhon doe\"\n",
    "f.history = \"Created \" + today.strftime(\"%d/%m/%y\")\n",
    "\n",
    "#Add local attributes to variable instances\n",
    "lon.units = 'degree_east'\n",
    "lat.units = 'degree_north'\n",
    "time.units = 'hours since '+(dates[0]+datetime.timedelta(hours=7)).strftime('%Y-%m-%d ')+str(cycle)+\":00:00\"\n",
    "rain.units = 'mm'\n",
    "\n",
    "# close file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"../repository/pre-processing/result-row\\ECMWF_new_3d.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "\n",
    "if (cycle == '00') :\n",
    "    n = 4\n",
    "else :\n",
    "    n = 0\n",
    "\n",
    "data = Dataset(data_path)\n",
    "print(data.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROSES UNTUK WMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check waktu data\n",
    "time = data.variables['time'][:]\n",
    "dates = num2date(time, data.variables['time'].units)\n",
    "print((dates[10+n]).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "print((dates[16+n]).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "\n",
    "lon_wms = np.array(data.variables[\"lon\"][:])\n",
    "lat_wms = np.array(data.variables[\"lat\"][:])\n",
    "prec_wms = np.array(data.variables[\"rain\"][:])\n",
    "\n",
    "# Inisialisasi akumulasi, meshgrid, tanggal\n",
    "xx, yy = np.meshgrid(lon, lat)\n",
    "accumulation = 0\n",
    "start_date = dates[11+n - 1].strftime('%m%d%Y')\n",
    "\n",
    "input_hour_dir = f\"../repository/post-processing/nc_to_tiff/pch_hour_{start_date}/\"\n",
    "input_day_dir = f\"../repository/post-processing/nc_to_tiff/pch_day_{start_date}/\"\n",
    "output_masked_hour_dir = f\"../repository/post-processing/wms/hour/pch_hour_{start_date}/\"\n",
    "output_masked_day_dir = f\"../repository/post-processing/wms/day/pch_day_{start_date}/\"\n",
    "\n",
    "os.makedirs(input_hour_dir, exist_ok=True)\n",
    "os.makedirs(input_day_dir, exist_ok=True)\n",
    "os.makedirs(output_masked_hour_dir, exist_ok=True)\n",
    "os.makedirs(output_masked_day_dir, exist_ok=True)\n",
    "\n",
    "for k in range(11+n, 19+n):\n",
    "    hour_data = prec[k, :, :]\n",
    "    accumulation += hour_data\n",
    "\n",
    "    start_hour = dates[k - 1].strftime('%H%M')\n",
    "\n",
    "    resolution_lon = (lon_wms.max() - lon_wms.min()) / lon_wms.shape[0]\n",
    "    resolution_lat = (lat_wms.max() - lat_wms.min()) / lat_wms.shape[0]\n",
    "    transform = from_origin(lon_wms.min(), lat_wms.max(), resolution_lon, resolution_lat)\n",
    "\n",
    "    tiff_filename_hour = os.path.join(input_hour_dir, f\"pch_hour_{start_date}_{start_hour}.tif\")\n",
    "\n",
    "    with rasterio.open(\n",
    "        tiff_filename_hour,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=hour_data.shape[0],\n",
    "        width=hour_data.shape[1],\n",
    "        count=1,\n",
    "        dtype=hour_data.dtype,\n",
    "        crs=rasterio.crs.CRS.from_proj4(\"+proj=longlat +datum=WGS84 +no_defs\"),\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(hour_data, 1)\n",
    "\n",
    "    print(f\"Successfully convert 3H netCDF to tiff: {tiff_filename_hour}\")\n",
    "\n",
    "tiff_filename_day = os.path.join(input_day_dir, f\"pch_day_{start_date}_2200.tif\")\n",
    "\n",
    "with rasterio.open(\n",
    "    tiff_filename_day,\n",
    "    'w',\n",
    "    driver='GTiff',\n",
    "    height=accumulation.shape[0],\n",
    "    width=accumulation.shape[1],\n",
    "    count=1,\n",
    "    dtype=accumulation.dtype,\n",
    "    crs=rasterio.crs.CRS.from_proj4(\"+proj=longlat +datum=WGS84 +no_defs\"),\n",
    "    transform=transform\n",
    ") as dst:\n",
    "    dst.write(accumulation, 1)\n",
    "\n",
    "print(f\"Successfully convert 1D netCDF to tiff: {tiff_filename_day}\")\n",
    "\n",
    "def masked_data(input_tiff, mask_file, output_masked_dir):\n",
    "    gdf = gpd.read_file(mask_file)\n",
    "    geometries = [geom for geom in gdf.geometry]\n",
    "\n",
    "    with rasterio.open(input_tiff) as src:\n",
    "        out_image, out_transform = mask(src, geometries, crop=True)\n",
    "        out_meta = src.meta.copy()\n",
    "\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": out_image.shape[1],\n",
    "        \"width\": out_image.shape[2],\n",
    "        \"transform\": out_transform\n",
    "    })\n",
    "\n",
    "    file_name = os.path.basename(input_tiff)\n",
    "    output_masked_tiff = os.path.join(output_masked_dir, file_name)\n",
    "\n",
    "    with rasterio.open(output_masked_tiff, \"w\", **out_meta) as dst:\n",
    "        dst.write(out_image)\n",
    "\n",
    "    print(f\"Result masked: {output_masked_tiff}\")\n",
    "\n",
    "for file in os.listdir(input_hour_dir):\n",
    "    if file.endswith(\".tif\"):\n",
    "        input_tiff = os.path.join(input_hour_dir, file)\n",
    "        masked_data(input_tiff, mask_pulau, output_masked_hour_dir)\n",
    "\n",
    "for file in os.listdir(input_day_dir):\n",
    "    if file.endswith(\".tif\"):\n",
    "        input_tiff = os.path.join(input_day_dir, file)\n",
    "        masked_data(input_tiff, mask_pulau, output_masked_day_dir)\n",
    "\n",
    "def delete_nc_to_tif(folder_path_for_tif):\n",
    "    try:\n",
    "        if os.path.exists(folder_path_for_tif):\n",
    "            shutil.rmtree(folder_path_for_tif)\n",
    "            print(f\"Successfully delete folder {folder_path_for_tif}\")\n",
    "        else:\n",
    "            print(f\"not found or previously deleted {folder_path_for_tif}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete folder {folder_path_for_tif}: {e}\")\n",
    "\n",
    "delete_nc_to_tif(input_hour_dir)\n",
    "delete_nc_to_tif(input_day_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROSES UNTUK WFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check waktu data\n",
    "time = data.variables['time'][:]\n",
    "dates = num2date(time, data.variables['time'].units)\n",
    "print((dates[10+n]).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "print((dates[16+n]).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "\n",
    "lat_wfs = data.variables[\"lat\"][:]\n",
    "lon_wfs = data.variables[\"lon\"][:]\n",
    "prec_wfs = data.variables['rain'][:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max.columns\",None)\n",
    "grid = pd.read_excel(path_pch_tabular)\n",
    "print (grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Data index lon, lat, balai, ws, kota\n",
    "grid_long = grid['idx_long'].to_numpy()\n",
    "grid_lat = grid['idx_lat'].to_numpy()\n",
    "longitude_r = grid['long_data']\n",
    "latitude_r = grid['lat_data']\n",
    "latitude_prod = grid['lat_prod']\n",
    "longitude_prod = grid['long_prod']\n",
    "pulau = grid['pulau']\n",
    "balai = grid['balai']\n",
    "kode_balai = grid['kode_balai']\n",
    "ws = grid ['wilayah_sungai']\n",
    "das = grid['das']\n",
    "prov = grid[\"provinsi\"]\n",
    "kota = grid['kabkot']\n",
    "wilayah = grid['wilayah']\n",
    "latshape = grid_lat.shape[0]\n",
    "latshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting 1 day ahead\n",
    "for k in range (11+n,19+n):\n",
    "    print((dates[k]).strftime(\"%Y%m%d%H\"))\n",
    "    idx_t=(dates[k]).strftime(\"%Y%m%d%H\")\n",
    "    if (k==19+n):\n",
    "        globals()['hujanharian_'+(idx_t)] = prec_wfs[11+n,:,:]\n",
    "    else:\n",
    "        globals()['hujanharian_'+(idx_t)] = prec_wfs[11+n:k+1,:,:].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting 2 days ahead\n",
    "for k in range (11+8+n,19+8+n):\n",
    "    print((dates[k]).strftime(\"%Y%m%d%H\"))\n",
    "    idx_t=(dates[k]).strftime(\"%Y%m%d%H\")\n",
    "    if (k==19+8+n):\n",
    "        globals()['hujanharian_'+(idx_t)] = prec_wfs[11+8+n,:,:]\n",
    "    else:\n",
    "        globals()['hujanharian_'+(idx_t)] = prec_wfs[11+8+n:k+1,:,:].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kolom = ['long_prod', 'lat_prod', 'tanggal', 'longitude','latitude','pulau', 'kode_balai', 'balai','das','provinsi','kabkot','wilayah']\n",
    "df_dasWaspada = pd.DataFrame(columns=kolom)\n",
    "for tab in range (latshape) :\n",
    "    gridlat = grid_lat[tab]\n",
    "    gridlon = grid_long[tab]\n",
    "    for k in range (11+n,27+n):\n",
    "        #utk cek awal mulai waspada\n",
    "        idx_t = (dates[k]).strftime(\"%Y%m%d%H\")\n",
    "        idx_h = (dates[k]).strftime(\"%H:00\")\n",
    "        hujan_cek = globals()['hujanharian_'+(idx_t)]\n",
    "        \n",
    "        #untuk tanggal status siaga banjir dan pengecekan status akhir siaga banjir di tiap grid\n",
    "        i_idx = 11+n if k<19+n else 19+n\n",
    "        tanggal = (dates[i_idx]).strftime(\"%d %B %Y\")\n",
    "        \n",
    "        if (hujan_cek[gridlat,gridlon]>=0.5):\n",
    "            df = pd.DataFrame([{'tanggal':tanggal, 'long_prod':longitude_prod[tab], 'lat_prod':latitude_prod[tab], 'longitude':longitude_r[tab],'latitude':latitude_r[tab], 'pulau':pulau[tab], 'kode_balai':kode_balai[tab], 'balai':balai[tab],\\\n",
    "                             'das':das[tab],'provinsi':prov[tab],'kabkot':kota[tab],'wilayah':wilayah[tab]\\\n",
    "                              ,'waktu_mulai':idx_h}])\n",
    "            i_idx = (11+n) if k<(19+n) else (19+n)\n",
    "            for i in range (i_idx,i_idx+8):\n",
    "                idx_t = (dates[i]).strftime(\"%Y%m%d%H\")\n",
    "                idx_h = (dates[i]).strftime(\"%H:00\")\n",
    "                df['ch_'+idx_h] = globals()['hujanharian_'+(idx_t)][gridlat,gridlon]\n",
    "            \n",
    "            kelas=globals()['hujanharian_'+(idx_t)][gridlat,gridlon]\n",
    "            if (0.5<kelas<=20):\n",
    "                status=\"1\" #HUJAN RINGAN\n",
    "            elif(20<kelas<=50):\n",
    "                status=\"2\" #HUJAN SEDANG\n",
    "            elif(50<kelas<=100):\n",
    "                status=\"3\" #HUJAN LEBAT\n",
    "            elif(100<kelas<=150):\n",
    "                status=\"4\" #HUJAN SANGAT LEBAT\n",
    "            elif(kelas>150):\n",
    "                status=\"5\" #HUJAN EKSTREM\n",
    "                \n",
    "            df[\"klasifikasi_hujan\"] = status\n",
    "            \n",
    "            status_cek = globals()['hujanharian_'+(idx_t)][gridlat,gridlon]\n",
    "            if (0.5<status_cek<=50):\n",
    "                status_1=\"1\" #AMAN\n",
    "            elif(50<status_cek<=75):\n",
    "                status_1=\"2\" #WASPADA\n",
    "            elif(75<status_cek<=100):\n",
    "                status_1=\"3\" #SIAGA\n",
    "            elif(status_cek>100):\n",
    "                status_1=\"4\" #AWAS\n",
    "            \n",
    "            df[\"status_akhir\"] = status_1\n",
    "            \n",
    "            df_dasWaspada = pd.concat([df_dasWaspada,df])\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "df = df_dasWaspada.sort_values(by=\"tanggal\")\n",
    "df = df.set_index(\"tanggal\")\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "print('Create accumulated data...')\n",
    "\n",
    "writer = pd.ExcelWriter('../repository/pre-processing/accumulation/accum_'+\\\n",
    "                        (dates[11+n]).strftime('%m%d%Y')+'_' #%Y%m%d\n",
    "                        +(dates[26+n]).strftime('%m%d%Y')+'.xlsx', engine='xlsxwriter')\n",
    "# Write each dataframe to a different worksheet.\n",
    "df.to_excel(writer, sheet_name='Akumulasi Berjalan')\n",
    "writer.close()\n",
    "\n",
    "print('Data accumulation has been completed...')\n",
    "\n",
    "print('Read accumulation data...')\n",
    "# data = pd.read_excel(\"../new-repository/pre-processing/accumulation/accum_01172025_01192025.xlsx\")\n",
    "data = pd.read_excel(\n",
    "    f\"../repository/pre-processing/accumulation/accum_{(dates[11+n]).strftime('%m%d%Y')}_{(dates[26+n]).strftime('%m%d%Y')}.xlsx\"\n",
    ")\n",
    "print(\"accumulation data value :\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "tanggal_hari_ini = datetime.now()\n",
    "tanggal_besok = tanggal_hari_ini + timedelta(days=1)\n",
    "\n",
    "tanggal_besok_str = tanggal_besok.strftime(\"%d %B %Y\")\n",
    "print(\"Tomorrow's date:\", tanggal_besok_str)\n",
    "\n",
    "data_tanggal_besok = data[data['tanggal'] == tanggal_besok_str]\n",
    "data_tanggal_besok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Result sorting descanding:\", tanggal_besok_str)\n",
    "data_tanggal_besok_sorted = data_tanggal_besok.sort_values(by='ch_01:00', ascending=False)\n",
    "data_tanggal_besok_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_remove_duplicate_tabular_data(data, subset_columns, output_folder, file_prefix, index_column):\n",
    "    print(f'Create tabular data for {file_prefix}...')\n",
    "\n",
    "    data_filtered = data.drop_duplicates(subset=subset_columns, keep='first')\n",
    "\n",
    "    remove_duplicate_tabular_data_path = os.path.join(output_folder, f\"{file_prefix}_{(dates[11+n]).strftime('%m%d%Y')}.xlsx\")\n",
    "\n",
    "    with pd.ExcelWriter(remove_duplicate_tabular_data_path, engine='xlsxwriter') as writer:\n",
    "        data_filtered.to_excel(writer, sheet_name=file_prefix.capitalize())\n",
    "\n",
    "    print(f'Tabular data for {file_prefix} has been completed and saved')\n",
    "\n",
    "    pivot = pd.pivot_table(\n",
    "        data_filtered,\n",
    "        index=index_column,\n",
    "        columns='klasifikasi_hujan',\n",
    "        values='kabkot',\n",
    "        aggfunc='count',\n",
    "        fill_value=0\n",
    "    )\n",
    "\n",
    "    print(pivot)\n",
    "\n",
    "create_remove_duplicate_tabular_data(\n",
    "    data=data_tanggal_besok_sorted,\n",
    "    subset_columns=['balai', 'kabkot'],\n",
    "    output_folder=\"../repository/pre-processing/remove-duplicate/balai/\",\n",
    "    file_prefix=\"remdup_balai\",\n",
    "    index_column=\"balai\"\n",
    ")\n",
    "\n",
    "create_remove_duplicate_tabular_data(\n",
    "    data=data_tanggal_besok_sorted,\n",
    "    subset_columns=['kabkot'],\n",
    "    output_folder=\"../repository/pre-processing/remove-duplicate/pulau/\",\n",
    "    file_prefix=\"remdup_pulau\",\n",
    "    index_column=\"pulau\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_data(remove_duplicate_path, get_columns, group_by, output_prediction, uniq_file_name):\n",
    "    print(f'Processing data for {uniq_file_name}...')\n",
    "\n",
    "    data_for_prediction = pd.read_excel(remove_duplicate_path)\n",
    "    prediction_data_filtered = data_for_prediction[get_columns]\n",
    "\n",
    "    add_columns = [\n",
    "        'total_kl_1', 'total_kl_2', 'total_kl_3', 'total_kl_4', 'total_kl_5',\n",
    "        'total_kg_1', 'total_kg_2', 'total_kg_3', 'total_kg_4',\n",
    "        'kelas_kl_1', 'kelas_kl_2', 'kelas_kl_3', 'kelas_kl_4', 'kelas_kl_5',\n",
    "        'kelas_kg_1', 'kelas_kg_2', 'kelas_kg_3', 'kelas_kg_4',\n",
    "        'last_data', 'last_updt'\n",
    "    ]\n",
    "\n",
    "    for col in add_columns:\n",
    "        prediction_data_filtered[col] = 0\n",
    "\n",
    "    # Menghitung jumlah total_kl_x per wilayah\n",
    "    for wilayah in prediction_data_filtered['wilayah'].unique():\n",
    "        wilayah_data = prediction_data_filtered[prediction_data_filtered['wilayah'] == wilayah]\n",
    "        for i in range(1, 6): \n",
    "            prediction_data_filtered.loc[prediction_data_filtered['wilayah'] == wilayah, f'total_kl_{i}'] = wilayah_data[wilayah_data['klasifikasi_hujan'] == i].shape[0]\n",
    "\n",
    "    # Menghitung jumlah total_kg_x per wilayah\n",
    "    for wilayah in prediction_data_filtered['wilayah'].unique():\n",
    "        wilayah_data = prediction_data_filtered[prediction_data_filtered['wilayah'] == wilayah]\n",
    "        for i in range(1, 5): \n",
    "            prediction_data_filtered.loc[prediction_data_filtered['wilayah'] == wilayah, f'total_kg_{i}'] = wilayah_data[wilayah_data['status_akhir'] == i].shape[0]\n",
    "\n",
    "    # Menghitung jumlah kelas_kl_x per grup (balai atau pulau)\n",
    "    for group in prediction_data_filtered[group_by].unique():\n",
    "        group_data = prediction_data_filtered[prediction_data_filtered[group_by] == group]\n",
    "        for i in range(1, 6): \n",
    "            prediction_data_filtered.loc[prediction_data_filtered[group_by] == group, f'kelas_kl_{i}'] = group_data[group_data['klasifikasi_hujan'] == i].shape[0]\n",
    "\n",
    "    # Menghitung jumlah kelas_kg_x per grup (balai atau pulau)\n",
    "    for group in prediction_data_filtered[group_by].unique():\n",
    "        group_data = prediction_data_filtered[prediction_data_filtered[group_by] == group]\n",
    "        for i in range(1, 5): \n",
    "            prediction_data_filtered.loc[prediction_data_filtered[group_by] == group, f'kelas_kg_{i}'] = group_data[group_data['status_akhir'] == i].shape[0]\n",
    "\n",
    "    # Tambahkan kolom last_data dan last_updt\n",
    "    prediction_data_filtered['last_data'] = filename\n",
    "    prediction_data_filtered['last_updt'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Hapus duplikasi berdasarkan kolom grup (balai atau pulau)\n",
    "    prediction_data_filtered = prediction_data_filtered.drop_duplicates(subset=[group_by])\n",
    "\n",
    "    # Tentukan path output\n",
    "    output_file = os.path.join(output_prediction, f\"{uniq_file_name}_{(dates[11+n]).strftime('%m%d%Y')}_2200.csv\")\n",
    "\n",
    "    # Simpan ke CSV\n",
    "    prediction_data_filtered.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f'Data processing for {uniq_file_name} completed')\n",
    "\n",
    "\n",
    "# **Proses Data untuk Balai**\n",
    "prediction_data(\n",
    "    remove_duplicate_path=f\"../repository/pre-processing/remove-duplicate/balai/remdup_balai_{(dates[11+n]).strftime('%m%d%Y')}.xlsx\",\n",
    "    get_columns=[\n",
    "        'tanggal', 'long_prod', 'lat_prod', 'longitude', 'latitude', 'wilayah', 'kode_balai', 'balai', 'ch_01:00',\n",
    "        'klasifikasi_hujan', 'status_akhir'\n",
    "    ],\n",
    "    group_by=\"balai\",\n",
    "    output_prediction=\"../repository/processing/day/balai/\",\n",
    "    uniq_file_name=\"balai_pch_day\"\n",
    ")\n",
    "\n",
    "# **Proses Data untuk Pulau**\n",
    "prediction_data(\n",
    "    remove_duplicate_path=f\"../repository/pre-processing/remove-duplicate/pulau/remdup_pulau_{(dates[11+n]).strftime('%m%d%Y')}.xlsx\",\n",
    "    get_columns=[\n",
    "        'tanggal', 'long_prod', 'lat_prod', 'longitude', 'latitude', 'wilayah', 'pulau', 'ch_01:00',\n",
    "        'klasifikasi_hujan', 'status_akhir'\n",
    "    ],\n",
    "    group_by=\"pulau\",\n",
    "    output_prediction=\"../repository/processing/day/pulau/\",\n",
    "    uniq_file_name=\"pulau_pch_day\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_shp(csv_file, output_wfs_dir, output_shp):\n",
    "\n",
    "    os.makedirs(output_wfs_dir, exist_ok=True)\n",
    "\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    columns_to_drop = ['ch_01:00', 'klasifikasi_hujan', 'status_akhir']\n",
    "    data = data.drop(columns=[col for col in columns_to_drop if col in data.columns], errors='ignore')\n",
    "\n",
    "    if 'latitude' in data.columns and 'longitude' in data.columns:\n",
    "        data['geometry'] = data.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "\n",
    "        gdf = gpd.GeoDataFrame(data, geometry='geometry')\n",
    "        gdf.set_crs(epsg=4326, inplace=True)\n",
    "        gdf.to_file(output_shp, driver='ESRI Shapefile')\n",
    "\n",
    "        print(f\"Successfully created vektor data {output_shp}\")\n",
    "    else:\n",
    "        print(\"Column 'latitude' or 'longitude' was not found in the CSV file\")\n",
    "\n",
    "start_date = (dates[11+n]).strftime('%m%d%Y')\n",
    "\n",
    "output_balai_dir = f\"../repository/post-processing/wfs/balai/pch_balai_{start_date}/\"\n",
    "output_pulau_dir = f\"../repository/post-processing/wfs/pulau/pch_pulau_{start_date}/\"\n",
    "\n",
    "csv_to_shp(\n",
    "    csv_file=f\"../repository/processing/day/balai/balai_pch_day_{start_date}_2200.csv\",\n",
    "    output_wfs_dir=output_balai_dir,\n",
    "    output_shp=f\"{output_balai_dir}balai_pch_day_{start_date}_2200.shp\"\n",
    ")\n",
    "\n",
    "# Panggil fungsi untuk Pulau\n",
    "csv_to_shp(\n",
    "    csv_file=f\"../repository/processing/day/pulau/pulau_pch_day_{start_date}_2200.csv\",\n",
    "    output_wfs_dir=output_pulau_dir,\n",
    "    output_shp=f\"{output_pulau_dir}pulau_pch_day_{start_date}_2200.shp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_dirs = [f\"../repository/post-processing/wfs/balai/pch_balai_{start_date}/\", f\"../repository/post-processing/wfs/pulau/pch_pulau_{start_date}/\"]\n",
    "tif_dirs = [f\"../repository/post-processing/wms/day/pch_day_{start_date}/\", f\"../repository/post-processing/wms/hour/pch_hour_{start_date}/\"]\n",
    "geoserver_endpoint = \"http://admin:geoserver@127.0.0.1:8080/geoserver\"\n",
    "workspace = \"demo_simadu\"\n",
    "\n",
    "def upload_to_geoserver(data_path, store_name, geoserver_endpoint, workspace):\n",
    "    file_extension = os.path.splitext(data_path)[1].lower()\n",
    "    if file_extension == \".shp\":\n",
    "        file_type = \"shp\"\n",
    "        store_type = \"datastores\"\n",
    "    elif file_extension == \".tif\":\n",
    "        file_type = \"geotiff\"\n",
    "        store_type = \"coveragestores\"\n",
    "    else:\n",
    "        print(f\"File type {file_extension} not supported for upload.\")\n",
    "        return False\n",
    "\n",
    "    absolute_path = os.path.abspath(data_path).replace(\"\\\\\", \"/\")\n",
    "    url = f\"{geoserver_endpoint}/rest/workspaces/{workspace}/{store_type}/{store_name}/external.{file_type}\"\n",
    "\n",
    "    headers = {\"Content-type\": \"text/plain\"}\n",
    "    response = requests.put(url, data=f\"file://{absolute_path}\", headers=headers, auth=(\"admin\", \"geoserver\"))\n",
    "\n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f\"Successfully uploaded {data_path} to geoserver\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to upload {data_path} to geoserver. Status code: {response.status_code}\")\n",
    "        return False\n",
    "\n",
    "def process_and_upload_to_geoserver(shp_dirs, tif_dirs, geoserver_endpoint, workspace):\n",
    "    for shp_dir in shp_dirs:\n",
    "        shp_files = [os.path.join(shp_dir, file) for file in os.listdir(shp_dir) if file.endswith('.shp')]\n",
    "        for shp_file in shp_files:\n",
    "            store_name = os.path.splitext(os.path.basename(shp_file))[0]\n",
    "            upload_to_geoserver(shp_file, store_name, geoserver_endpoint, workspace)\n",
    "\n",
    "    for tif_dir in tif_dirs:\n",
    "        tif_files = [os.path.join(tif_dir, file) for file in os.listdir(tif_dir) if file.endswith('.tif')]\n",
    "        for tif_file in tif_files:\n",
    "            store_name = os.path.splitext(os.path.basename(tif_file))[0]\n",
    "            upload_to_geoserver(tif_file, store_name, geoserver_endpoint, workspace)\n",
    "\n",
    "# Jalankan proses\n",
    "process_and_upload_to_geoserver(\n",
    "    shp_dirs,\n",
    "    tif_dirs,\n",
    "    geoserver_endpoint,\n",
    "    workspace,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
