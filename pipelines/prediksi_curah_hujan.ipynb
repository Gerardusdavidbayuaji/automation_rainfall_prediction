{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ftplib\n",
    "import datetime\n",
    "from netCDF4 import Dataset, num2date\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path_nc_file = \"../data/row\"\n",
    "path_nc_row = \"../repository/pre-processing/row\"\n",
    "\n",
    "# konfigurasi ftp\n",
    "ftp_host = os.getenv(\"HOST\")\n",
    "ftp_user = os.getenv(\"USER\")\n",
    "ftp_password = os.getenv(\"PASSWORD\")\n",
    "cycle = \"12\"\n",
    "\n",
    "# FTP functions\n",
    "def connect_ftp():\n",
    "    ftp = ftplib.FTP(ftp_host)\n",
    "    ftp.login(ftp_user, ftp_password)\n",
    "    ftp.cwd(\"/\")\n",
    "    return ftp\n",
    "\n",
    "def download_file_from_ftp(ftp, filename):\n",
    "    try:\n",
    "        file_list = ftp.nlst()\n",
    "        if filename in file_list:\n",
    "            local_file_path = os.path.join(path_nc_file, filename)\n",
    "            if not os.path.exists(local_file_path):\n",
    "                with open(local_file_path, \"wb\") as local_file:\n",
    "                    ftp.retrbinary(f\"RETR {filename}\", local_file.write)\n",
    "                print(f\"Berhasil download file {filename}\")\n",
    "            else:\n",
    "                print(f\"File {filename} sudah tersedia\")\n",
    "            return local_file_path\n",
    "    except Exception:\n",
    "        print(\"File rusak, tidak ada yang bisa dilakukan\")\n",
    "        return None\n",
    "\n",
    "def download_latest_file_from_ftp(ftp):\n",
    "    file_list = ftp.nlst()\n",
    "    if file_list:\n",
    "        latest_file = sorted(file_list)[-1]\n",
    "        return download_file_from_ftp(ftp, latest_file)\n",
    "    return None\n",
    "\n",
    "# Download file .nc\n",
    "today = datetime.date.today() - datetime.timedelta(days=1)\n",
    "filename = f\"ECMWF.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "print(\"Sedang mengunduh:\", filename)\n",
    "\n",
    "ftp = connect_ftp()\n",
    "if ftp:\n",
    "    local_file_path = download_file_from_ftp(ftp, filename) or download_latest_file_from_ftp(ftp)\n",
    "    ftp.quit()\n",
    "else:\n",
    "    print(\"Tidak dapat terhubung ke server FTP.\")\n",
    "\n",
    "if local_file_path is None:\n",
    "    print(\"File tidak tersedia untuk didownload\")\n",
    "    exit()\n",
    "\n",
    "# Baca data .nc\n",
    "data = Dataset(local_file_path)\n",
    "\n",
    "lat = data.variables['lat'][:]\n",
    "lon = data.variables['lon'][:]\n",
    "prec = data.variables['tp'][:,0,:,:]\n",
    "time = data.variables['time'][:]\n",
    "\n",
    "dates = num2date(time, data.variables['time'].units)\n",
    "print((dates[7]+datetime.timedelta(hours=7)).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "print((dates[15]+datetime.timedelta(hours=7)).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "\n",
    "xrain = prec\n",
    "print(np.array_equal(xrain,prec))\n",
    "\n",
    "#change rainfall data\n",
    "for time in range (len(dates)):\n",
    "    for lati in range(len(lat)) :\n",
    "        for loni in range (len(lon)) :\n",
    "            if (time<=0) :\n",
    "                if (xrain[time,lati,loni]<=0) :\n",
    "                    xrain[time,lati,loni] == 0\n",
    "            elif(time>0) :\n",
    "                if (xrain[time,lati,loni]<0) :\n",
    "                    xrain[time,lati,loni] = xrain[time-1,lati,loni]\n",
    "                if (xrain[time,lati,loni]-xrain[time-1,lati,loni]<0) :\n",
    "                    xrain[time,lati,loni] = xrain[time-1,lati,loni]\n",
    "\n",
    "# initiate rain before changing rainfall accumulate to interval rainfall\n",
    "hjn=np.empty((len(dates),len(lat),len(lon)))\n",
    "# change rainfall accumulate to interval rainfall\n",
    "hjn[0,:,:]=xrain[0,:,:]\n",
    "for i in range (1,len(dates)) :\n",
    "    hjn[i,:,:] = xrain[i,:,:]-xrain[i-1,:,:]\n",
    "\n",
    "# reshape 3d to 4d so it can be sabed into netcdf permanent dimension\n",
    "hjn2=hjn.reshape(len(dates), 1, len(lat), len(lon))\n",
    "\n",
    "# rewrite to netcdf\n",
    "ds=xr.open_dataset(local_file_path)\n",
    "ds['tp'].values=hjn2\n",
    "ds=ds.assign_coords(time=(\"time\",ds['time'].values + np.timedelta64(7,'h')))\n",
    "\n",
    "# output rewrite to netcdf\n",
    "output_rewrite = f\"ECMWF_new.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "output_path = os.path.join(path_nc_row, output_rewrite)\n",
    "ds.to_netcdf(output_path)\n",
    "print (ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenNetCDFFile\n",
    "datanew = Dataset(path_nc_row+\"/ECMWF_new.0125.\"+today.strftime('%Y%m%d')+cycle+\"00.PREC.nc\")\n",
    "#Rainfall data(time, lat, lon)\n",
    "prec_new = datanew.variables['tp'][:,0,:,:]\n",
    "plt.plot(prec_new[:,80,120])\n",
    "plt.plot(hjn[:,80,120])\n",
    "print (\"hasil ketepatan modifikasi nc \\n\",np.corrcoef(prec_new[:,80,120],hjn[:,80,120]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_modified = \"../repository/pre-processing/result-row\"\n",
    "result_file_name = f\"ECMWF_new_3d.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "\n",
    "# Gabungkan path dengan nama file\n",
    "file_path = os.path.join(path_modified, result_file_name)\n",
    "\n",
    "# Buat file NetCDF baru\n",
    "f = Dataset(file_path, 'w', format='NETCDF4')\n",
    "print(f\"File {result_file_name} berhasil dibuat\")\n",
    "print (data.variables)\n",
    "\n",
    "# define variables foe new netcdf4 file\n",
    "rain=hjn[:48,:,:]\n",
    "latitude=data.variables[\"lat\"][:]\n",
    "longitude=data.variables[\"lon\"][:]\n",
    "time_k = data.variables['time'][:48]\n",
    "\n",
    "tempgrp = f.createGroup('Rain_data')\n",
    "\n",
    "# Create dimension for netCDF4\n",
    "f.createDimension('lon', len(longitude))\n",
    "f.createDimension('lat', len(latitude))\n",
    "f.createDimension('time', len(time_k))\n",
    "\n",
    "# Create variables for netCDF4\n",
    "lon = f.createVariable('lon', 'f4', 'lon')\n",
    "lat = f.createVariable('lat', 'f4', 'lat')  \n",
    "rain = f.createVariable('rain', 'f4', ('time', 'lat', 'lon'))\n",
    "time = f.createVariable('time', 'i4', 'time')\n",
    "\n",
    "# define variables to be saved into netcdf4 file\n",
    "lon[:] = longitude[:] #The \"[:]\" at the end of the variable instance is necessary\n",
    "lat[:] = latitude[:]\n",
    "rain[:,:,:] = hjn[:48,:,:]\n",
    "time[:]=time_k+7\n",
    "\n",
    "print (dates[0].strftime('%Y-%m-%d ')+str(cycle)+\":00:00\")\n",
    "\n",
    "#Add global attributes\n",
    "f.description = \"ECMWF from BMKG modified by Jhon doe\"\n",
    "f.history = \"Created \" + today.strftime(\"%d/%m/%y\")\n",
    "\n",
    "#Add local attributes to variable instances\n",
    "lon.units = 'degree_east'\n",
    "lat.units = 'degree_north'\n",
    "time.units = 'hours since '+(dates[0]+datetime.timedelta(hours=7)).strftime('%Y-%m-%d ')+str(cycle)+\":00:00\"\n",
    "rain.units = 'mm'\n",
    "\n",
    "nilai_hujan = f.variables['rain'][:]\n",
    "print('nilai curah hujan', nilai_hujan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PEMROSESAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = '../repository/pre-processing/result-row\\ECMWF_new_3d.0125.202501151200.PREC.nc'\n",
    "n=0\n",
    "\n",
    "data = Dataset(data_path)\n",
    "data.variables\n",
    "print('value', data.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate data lon and lat\n",
    "lat=data.variables[\"lat\"][:]\n",
    "lon=data.variables[\"lon\"][:]\n",
    "\n",
    "#Rainfall data(time, lat, lon)\n",
    "prec = data.variables['rain'][:,:,:]\n",
    "\n",
    "#Check waktu data\n",
    "time = data.variables['time'][:]\n",
    "dates = num2date(time, data.variables['time'].units)\n",
    "print((dates[10+n]).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "print((dates[16+n]).strftime('%Y-%m-%d %H:%M:%S WIB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open Basin Grid Index\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max.columns\",None)\n",
    "grid = pd.read_excel('../data/tabular/data_pch_balai.xlsx')\n",
    "print (grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Data index lon, lat, balai, ws, kota\n",
    "grid_long=grid['idx_long'].to_numpy()\n",
    "grid_lat=grid['idx_lat'].to_numpy()\n",
    "longitude_r = grid['long_data']\n",
    "latitude_r = grid['lat_data']\n",
    "latitude_prod = grid['lat_prod']\n",
    "longitude_prod = grid['long_prod']\n",
    "pulau = grid['pulau']\n",
    "balai= grid['balai']\n",
    "ws = grid ['wilayah_sungai']\n",
    "das = grid['das']\n",
    "prov=grid[\"provinsi\"]\n",
    "kota = grid['kabkot']\n",
    "wilayah=grid['wilayah']\n",
    "latshape = grid_lat.shape[0]\n",
    "latshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting 1 day ahead\n",
    "for k in range (11+n,19+n):\n",
    "    print((dates[k]).strftime(\"%Y%m%d%H\"))\n",
    "    idx_t=(dates[k]).strftime(\"%Y%m%d%H\")\n",
    "    if (k==19+n):\n",
    "        globals()['hujanharian_'+(idx_t)] = prec[11+n,:,:]\n",
    "    else:\n",
    "        globals()['hujanharian_'+(idx_t)] = prec[11+n:k+1,:,:].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting 2 days ahead\n",
    "for k in range (11+8+n,19+8+n):\n",
    "    print((dates[k]).strftime(\"%Y%m%d%H\"))\n",
    "    idx_t=(dates[k]).strftime(\"%Y%m%d%H\")\n",
    "    if (k==19+8+n):\n",
    "        globals()['hujanharian_'+(idx_t)] = prec[11+8+n,:,:]\n",
    "    else:\n",
    "        globals()['hujanharian_'+(idx_t)] = prec[11+8+n:k+1,:,:].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kolom=['long_prod', 'lat_prod', 'tanggal', 'longitude','latitude','pulau','balai','das','provinsi','kabkot','wilayah']\n",
    "df_dasWaspada=pd.DataFrame(columns=kolom)\n",
    "for tab in range (latshape) :\n",
    "    gridlat = grid_lat[tab]\n",
    "    gridlon = grid_long[tab]\n",
    "    for k in range (11+n,27+n):\n",
    "        #utk cek awal mulai waspada\n",
    "        idx_t=(dates[k]).strftime(\"%Y%m%d%H\")\n",
    "        idx_h=(dates[k]).strftime(\"%H:00\")\n",
    "        hujan_cek=globals()['hujanharian_'+(idx_t)]\n",
    "        \n",
    "        #untuk tanggal status siaga banjir dan pengecekan status akhir siaga banjir di tiap grid\n",
    "        i_idx=11+n if k<19+n else 19+n\n",
    "        tanggal=(dates[i_idx]).strftime(\"%d %B %Y\")\n",
    "        \n",
    "        if (hujan_cek[gridlat,gridlon]>=0.5):\n",
    "            df=pd.DataFrame([{'tanggal':tanggal, 'long_prod':longitude_prod[tab], 'lat_prod':latitude_prod[tab], 'longitude':longitude_r[tab],'latitude':latitude_r[tab], 'pulau':pulau[tab], 'balai':balai[tab],\\\n",
    "                             'das':das[tab],'provinsi':prov[tab],'kabkot':kota[tab],'wilayah':wilayah[tab]\\\n",
    "                              ,'waktu_mulai':idx_h}])\n",
    "            i_idx=(11+n) if k<(19+n) else (19+n)\n",
    "            for i in range (i_idx,i_idx+8):\n",
    "                idx_t=(dates[i]).strftime(\"%Y%m%d%H\")\n",
    "                idx_h=(dates[i]).strftime(\"%H:00\")\n",
    "                df['ch_'+idx_h]=globals()['hujanharian_'+(idx_t)][gridlat,gridlon]\n",
    "            \n",
    "            kelas=globals()['hujanharian_'+(idx_t)][gridlat,gridlon]\n",
    "            if (0.5<kelas<=20):\n",
    "                status=\"1\" #HUJAN RINGAN\n",
    "            elif(20<kelas<=50):\n",
    "                status=\"2\" #HUJAN SEDANG\n",
    "            elif(50<kelas<=100):\n",
    "                status=\"3\" #HUJAN LEBAT\n",
    "            elif(100<kelas<=150):\n",
    "                status=\"4\" #HUJAN SANGAT LEBAT\n",
    "            elif(kelas>150):\n",
    "                status=\"5\" #HUJAN EKSTREM\n",
    "                \n",
    "            df[\"klasifikasi_hujan\"]=status\n",
    "            \n",
    "            status_cek=globals()['hujanharian_'+(idx_t)][gridlat,gridlon]\n",
    "            if (0.5<status_cek<=50):\n",
    "                status_1=\"1\" #AMAN\n",
    "            elif(50<status_cek<=75):\n",
    "                status_1=\"2\" #WASPADA\n",
    "            elif(75<status_cek<=100):\n",
    "                status_1=\"3\" #SIAGA\n",
    "            elif(status_cek>100):\n",
    "                status_1=\"4\" #AWAS\n",
    "            \n",
    "            df[\"status_akhir\"]=status_1\n",
    "            \n",
    "            df_dasWaspada=pd.concat([df_dasWaspada,df])\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "df=df_dasWaspada.sort_values(by=\"tanggal\")\n",
    "df=df.set_index(\"tanggal\")\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('../repository/pre-processing/accumulation/accum_'+\\\n",
    "                        (dates[11+n]).strftime('%m%d%Y')+'_' #%Y%m%d\n",
    "                        +(dates[26+n]).strftime('%m%d%Y')+'.xlsx', engine='xlsxwriter')\n",
    "# Write each dataframe to a different worksheet.\n",
    "df.to_excel(writer, sheet_name='Akumulasi Berjalan')\n",
    "writer.close()\n",
    "\n",
    "print('Done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"../repository/pre-processing/accumulation/accum_01172025_01192025.xlsx\")\n",
    "# data['tanggal'] = pd.to_timedelta(data['tanggal'])\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "tanggal_hari_ini = datetime.now()\n",
    "tanggal_besok = tanggal_hari_ini + timedelta(days=1)\n",
    "\n",
    "# Formatkan tanggal besok ke string\n",
    "tanggal_besok_str = tanggal_besok.strftime(\"%d %B %Y\")\n",
    "print(\"Tanggal besok:\", tanggal_besok_str)\n",
    "\n",
    "data_tanggal_besok = data[data['tanggal'] == tanggal_besok_str]\n",
    "data_tanggal_besok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_urut = data.sort_values(by=['ch_01:00'], ascending=False)\n",
    "# Sort data berdasarkan ch_01.00\n",
    "\n",
    "data_tanggal_besok_sorted = data_tanggal_besok.sort_values(by='ch_01:00', ascending=False)\n",
    "data_tanggal_besok_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_remove_balai = data_tanggal_besok_sorted.drop_duplicates(subset=['balai', 'kabkot'], keep='first')\n",
    "print('cek data tabular', data_remove_balai)\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('../repository/pre-processing/remove-duplicate/balai/remdup_balai_'+\\\n",
    "                        (dates[11+n]).strftime('%m%d%Y')+'.xlsx', engine='xlsxwriter')\n",
    "# Write each dataframe to a different worksheet.\n",
    "data_remove_balai.to_excel(writer, sheet_name='Balai')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.close()\n",
    "\n",
    "print('Done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = pd.pivot_table(\n",
    "    data_remove_balai,\n",
    "    index='balai',       # Baris pivot table\n",
    "    columns='klasifikasi_hujan',    # Kolom pivot table\n",
    "    values='kabkot',     # Kolom yang akan dihitung\n",
    "    aggfunc='count',        # Fungsi agregasi\n",
    "    fill_value=0            # Mengisi nilai NaN dengan 0\n",
    ")\n",
    "\n",
    "# Hasil\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_remove_kab = data_tanggal_besok_sorted.drop_duplicates(subset=['kabkot'], keep='first')\n",
    "#Ini yang untuk grafik perpulau, tinggal diklasifikasikan saja berdasarkan pulau. begitu juga untuk tabel kasifikasi berdasarkan wilayah\n",
    "print('cek data tabular',data_remove_kab)\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('../repository/pre-processing/remove-duplicate/pulau/remdup_pulau_'+\\\n",
    "                        (dates[11+n]).strftime('%m%d%Y')+'.xlsx', engine='xlsxwriter')\n",
    "# Write each dataframe to a different worksheet.\n",
    "data_remove_kab.to_excel(writer, sheet_name='Pulau')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.close()\n",
    "\n",
    "print('Done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = pd.pivot_table(\n",
    "    data_remove_balai,\n",
    "    index='pulau',       # Baris pivot table\n",
    "    columns='klasifikasi_hujan',    # Kolom pivot table\n",
    "    values='kabkot',     # Kolom yang akan dihitung\n",
    "    aggfunc='count',        # Fungsi agregasi\n",
    "    fill_value=0            # Mengisi nilai NaN dengan 0\n",
    ")\n",
    "\n",
    "# Hasil\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TABEL UNTUK BALAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balai_remdup_path = '../repository/pre-processing/remove-duplicate/balai/remdup_balai_'+(dates[11+n]).strftime('%m%d%Y')+'.xlsx'\n",
    "data = pd.read_excel(balai_remdup_path)\n",
    "\n",
    "get_columns_from_balai_remdup = [\n",
    "    'tanggal', 'long_prod', 'lat_prod', 'longitude', 'latitude', 'wilayah', 'balai', 'ch_01:00', 'klasifikasi_hujan', 'status_akhir'\n",
    "]\n",
    "\n",
    "data_filtered_by_balai_remdup = data[get_columns_from_balai_remdup]\n",
    "\n",
    "add_columns = [\n",
    "    'total_kl_1', 'total_kl_2', 'total_kl_3', 'total_kl_4', 'total_kl_5',\n",
    "    'total_kg_1', 'total_kg_2', 'total_kg_3', 'total_kg_4',\n",
    "    'kelas_kl_1', 'kelas_kl_2', 'kelas_kl_3', 'kelas_kl_4', 'kelas_kl_5',\n",
    "    'kelas_kg_1', 'kelas_kg_2', 'kelas_kg_3', 'kelas_kg_4',\n",
    "    'last_data', 'last_updt'\n",
    "]\n",
    "\n",
    "for col in add_columns:\n",
    "    data_filtered_by_balai_remdup[col] = 0\n",
    "\n",
    "# menghitung jumlah total_kl_1 sampai total_kl_5 per wilayah\n",
    "for wilayah in data_filtered_by_balai_remdup['wilayah'].unique():\n",
    "    wilayah_data = data_filtered_by_balai_remdup[data_filtered_by_balai_remdup['wilayah'] == wilayah]\n",
    "    for i in range (1, 6): \n",
    "        data_filtered_by_balai_remdup.loc[data_filtered_by_balai_remdup['wilayah'] == wilayah, f'total_kl_{i}'] = wilayah_data[wilayah_data['klasifikasi_hujan'] == i].shape[0]\n",
    "\n",
    "# menghitung jumlah total_kg_1 sampai total_kg_4 per wilayah\n",
    "for wilayah in data_filtered_by_balai_remdup['wilayah'].unique():\n",
    "    wilayah_data = data_filtered_by_balai_remdup[data_filtered_by_balai_remdup['wilayah'] == wilayah]\n",
    "    for i in range (1, 5): \n",
    "        data_filtered_by_balai_remdup.loc[data_filtered_by_balai_remdup['wilayah'] == wilayah, f'total_kg_{i}'] = wilayah_data[wilayah_data['status_akhir'] == i].shape[0]\n",
    "\n",
    "# menghitung jumlah kelas_kl_1 sampai kelas_kl_5 per balai\n",
    "for balai in data_filtered_by_balai_remdup['balai'].unique():\n",
    "    balai_data = data_filtered_by_balai_remdup[data_filtered_by_balai_remdup['balai'] == balai]\n",
    "    for i in range (1, 6): \n",
    "        data_filtered_by_balai_remdup.loc[data_filtered_by_balai_remdup['balai'] == balai, f'kelas_kl_{i}'] = balai_data[balai_data['klasifikasi_hujan'] == i].shape[0]\n",
    "\n",
    "# menghitung jumlah kelas_kg_1 sampai kelas_kg_4 per balai\n",
    "for balai in data_filtered_by_balai_remdup['balai'].unique():\n",
    "    balai_data = data_filtered_by_balai_remdup[data_filtered_by_balai_remdup['balai'] == balai]\n",
    "    for i in range (1, 5): \n",
    "        data_filtered_by_balai_remdup.loc[data_filtered_by_balai_remdup['balai'] == balai, f'kelas_kg_{i}'] = balai_data[balai_data['status_akhir'] == i].shape[0]\n",
    "\n",
    "data_filtered_by_balai_remdup['last_data'] = filename\n",
    "data_filtered_by_balai_remdup['last_updt'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "data_filtered_by_balai_remdup = data_filtered_by_balai_remdup.drop_duplicates(subset=['balai'])        \n",
    "\n",
    "output_balai_day = '../repository/processing/day/balai/balai_pch_day_'+(dates[11+n]).strftime('%m%d%Y')+'_2200.csv'\n",
    "data_filtered_by_balai_remdup.to_csv(output_balai_day, index=False)\n",
    "\n",
    "print('done...')\n",
    "\n",
    "writer = pd.ExcelWriter('../repository/pre-processing/remove-duplicate/pulau/remdup_pulau_'+\\\n",
    "                        (dates[11+n]).strftime('%m%d%Y')+'.xlsx', engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TABEL UNTUK PULAU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulau_remdup_path = '../repository/pre-processing/remove-duplicate/pulau/remdup_pulau_'+(dates[11+n]).strftime('%m%d%Y')+'.xlsx'\n",
    "data = pd.read_excel(pulau_remdup_path)\n",
    "\n",
    "get_columns_from_remdup = [\n",
    "    'tanggal', 'long_prod', 'lat_prod', 'longitude', 'latitude', 'wilayah', 'pulau', 'ch_01:00', 'klasifikasi_hujan', 'status_akhir'\n",
    "]\n",
    "\n",
    "data_filtered_by_pulau_remdup = data[get_columns_from_remdup]\n",
    "# print(get_columns_from_remdup)\n",
    "\n",
    "add_columns = [\n",
    "    'total_kl_1', 'total_kl_2', 'total_kl_3', 'total_kl_4', 'total_kl_5',\n",
    "    'total_kg_1', 'total_kg_2', 'total_kg_3', 'total_kg_4',\n",
    "    'kelas_kl_1', 'kelas_kl_2', 'kelas_kl_3', 'kelas_kl_4', 'kelas_kl_5',\n",
    "    'kelas_kg_1', 'kelas_kg_2', 'kelas_kg_3', 'kelas_kg_4',\n",
    "    'last_data', 'last_updt'\n",
    "]\n",
    "\n",
    "for col in add_columns:\n",
    "    data_filtered_by_pulau_remdup[col] = 0\n",
    "\n",
    "# menghitung jumlah total_kl_1 sampai total_kl_5 per wilayah\n",
    "for wilayah in data_filtered_by_pulau_remdup['wilayah'].unique():\n",
    "    wilayah_data = data_filtered_by_pulau_remdup[data_filtered_by_pulau_remdup['wilayah'] == wilayah]\n",
    "    for i in range (1, 6): \n",
    "        data_filtered_by_pulau_remdup.loc[data_filtered_by_pulau_remdup['wilayah'] == wilayah, f'total_kl_{i}'] = wilayah_data[wilayah_data['klasifikasi_hujan'] == i].shape[0]\n",
    "\n",
    "# menghitung jumlah total_kg_1 sampai total_kg_4 per wilayah\n",
    "for wilayah in data_filtered_by_pulau_remdup['wilayah'].unique():\n",
    "    wilayah_data = data_filtered_by_pulau_remdup[data_filtered_by_pulau_remdup['wilayah'] == wilayah]\n",
    "    for i in range (1, 5): \n",
    "        data_filtered_by_pulau_remdup.loc[data_filtered_by_pulau_remdup['wilayah'] == wilayah, f'total_kg_{i}'] = wilayah_data[wilayah_data['status_akhir'] == i].shape[0]\n",
    "\n",
    "# menghitung jumlah kelas_kl_1 sampai kelas_kl_5 per pulau\n",
    "for pulau in data_filtered_by_pulau_remdup['pulau'].unique():\n",
    "    pulau_data = data_filtered_by_pulau_remdup[data_filtered_by_pulau_remdup['pulau'] == pulau]\n",
    "    for i in range (1, 6): \n",
    "        data_filtered_by_pulau_remdup.loc[data_filtered_by_pulau_remdup['pulau'] == pulau, f'kelas_kl_{i}'] = pulau_data[pulau_data['klasifikasi_hujan'] == i].shape[0]\n",
    "\n",
    "# menghitung jumlah kelas_kg_1 sampai kelas_kg_4 per pulau\n",
    "for pulau in data_filtered_by_pulau_remdup['pulau'].unique():\n",
    "    pulau_data = data_filtered_by_pulau_remdup[data_filtered_by_pulau_remdup['pulau'] == pulau]\n",
    "    for i in range (1, 5): \n",
    "        data_filtered_by_pulau_remdup.loc[data_filtered_by_pulau_remdup['pulau'] == pulau, f'kelas_kg_{i}'] = pulau_data[pulau_data['status_akhir'] == i].shape[0]\n",
    "\n",
    "data_filtered_by_pulau_remdup['last_data'] = filename\n",
    "data_filtered_by_pulau_remdup['last_updt'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "data_filtered_by_pulau_remdup = data_filtered_by_pulau_remdup.drop_duplicates(subset=['pulau'])        \n",
    "\n",
    "output_pulau_day = '../repository/processing/day/pulau/pulau_pch_day_'+(dates[11+n]).strftime('%m%d%Y')+'_2200.csv'\n",
    "data_filtered_by_pulau_remdup.to_csv(output_pulau_day, index=False)\n",
    "\n",
    "print('done...')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PASCA PEMROSESAN\n",
    "\n",
    "1. ubah csv jadi vektor .shp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "csv_to_vektor = '../repository/processing/day/balai/balai_pch_day_'+(dates[11+n]).strftime('%m%d%Y')+'_2200.csv'\n",
    "# csv_to_vektor = '../repository/processing/day/pulau/pulau_pch_day_'+(dates[11+n]).strftime('%m%d%Y')+'_2200.csv'\n",
    "\n",
    "data = pd.read_csv(csv_to_vektor)\n",
    "\n",
    "if 'latitude' in data.columns and 'longitude' in data.columns:\n",
    "    data['geometry'] = data.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(data, geometry='geometry')\n",
    "\n",
    "    gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "    save_vektor = '../repository/post-processing/csv-to-vektor/balai/balai_pch_day_'+(dates[11+n]).strftime('%m%d%Y')+'_2200.shp'\n",
    "    # save_vektor = '../repository/post-processing/csv-to-vektor/pulau/pulau_pch_day_'+(dates[11+n]).strftime('%m%d%Y')+'_2200.shp'\n",
    "\n",
    "\n",
    "    gdf.to_file(save_vektor, driver='ESRI Shapefile')\n",
    "\n",
    "    print('data vektor berhasil dibuat')\n",
    "else:\n",
    "    print('gagal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. VISUALISASI PREDIKSI CURAH HUJAN PER 3 JAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "data_path = '../repository/pre-processing/result-row\\ECMWF_new_3d.0125.202501151200.PREC.nc'\n",
    "n=0\n",
    "\n",
    "data_3d = Dataset(data_path)\n",
    "data_3d.variables\n",
    "print('value', data_3d.variables)\n",
    "\n",
    "#Check waktu data_3d\n",
    "time_3d = data_3d.variables['time'][:]\n",
    "dates_3d = num2date(time_3d, data_3d.variables['time'].units)\n",
    "print((dates_3d[11+n]).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "print((dates_3d[19+n]).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "\n",
    "#Get long Lat data_3d\n",
    "lon_3d = data_3d.variables['lon']\n",
    "lat_3d = data_3d.variables['lat']\n",
    "\n",
    "#Rainfall data_3d(time, lat, lon)\n",
    "prec_3d = data_3d.variables['rain'][:,:,:]\n",
    "\n",
    "if (cycle == '00'):\n",
    "    n = 4\n",
    "else: \n",
    "    n = 0\n",
    "\n",
    "output_hour = '../repository/processing/hour/' \n",
    "\n",
    "for k in range(11 + n, 19 + n):\n",
    "    start_date = dates_3d[k - 1].strftime('%m%d%Y')  # Format tanggal: MMDDYYYY\n",
    "    start_hour = dates_3d[k - 1].strftime('%H%M')  # Jam mulai: HHMM\n",
    "    end_hour = dates_3d[k].strftime('%H%M')  # Jam akhir: HHMM\n",
    "\n",
    "    prec_slice = prec_3d[k, :, :]\n",
    "    lon_grid, lat_grid = np.meshgrid(lon_3d, lat_3d)\n",
    "    data_dict = {\n",
    "        'latitude': lat_grid.flatten(),\n",
    "        'longitude': lon_grid.flatten(),\n",
    "        'prec': prec_slice.flatten(),\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data_dict)\n",
    "\n",
    "    csv_filename = f\"{output_hour}pch_hour_{start_date}_{start_hour}_{end_hour}.csv\"\n",
    "\n",
    "    # Menambahkan baris metadata untuk rentang waktu dan menyimpan file CSV\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "    print(f\"Data {csv_filename} berhasil disimpan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. VISUALISASI PREDIKSI CURAH HUJAN PER HARI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset, num2date\n",
    "\n",
    "# Path data dan inisialisasi variabel\n",
    "data_path = '../repository/pre-processing/result-row\\ECMWF_new_3d.0125.202501151200.PREC.nc'\n",
    "\n",
    "n = 0  # Sesuaikan dengan siklus waktu\n",
    "cycle = '00'\n",
    "\n",
    "# Load data NetCDF\n",
    "data_3d = Dataset(data_path)\n",
    "\n",
    "# Ekstrak waktu, latitude, longitude, dan presipitasi\n",
    "time_3d = data_3d.variables['time'][:]\n",
    "dates_3d = num2date(time_3d, data_3d.variables['time'].units)\n",
    "lon_3d = data_3d.variables['lon'][:]\n",
    "lat_3d = data_3d.variables['lat'][:]\n",
    "prec_3d = data_3d.variables['rain'][:,:,:]\n",
    "\n",
    "# Update nilai n jika diperlukan\n",
    "if cycle == '00':\n",
    "    n = 4\n",
    "else:\n",
    "    n = 0\n",
    "\n",
    "output_daily = '../repository/processing/day/csv-to-idw/' \n",
    "\n",
    "# Akumulasi data_3d presipitasi per hari\n",
    "start_date = dates_3d[11 + n].strftime('%m%d%Y')\n",
    "end_date = dates_3d[19 + n - 1].strftime('%m%d%Y')\n",
    "akumulasi = np.zeros_like(prec_3d[0, :, :])\n",
    "\n",
    "for k in range(11 + n, 19 + n):\n",
    "    akumulasi += prec_3d[k, :, :]\n",
    "\n",
    "# Membuat grid latitude dan longitude\n",
    "lon_grid, lat_grid = np.meshgrid(lon_3d, lat_3d)\n",
    "\n",
    "# Membuat DataFrame dari data akumulasi\n",
    "data_dict = {\n",
    "    'latitude': lat_grid.flatten(),\n",
    "    'longitude': lon_grid.flatten(),\n",
    "    'prec': akumulasi.flatten(),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Menyimpan file CSV dengan akumulasi per hari\n",
    "csv_filename = f\"{output_daily}pch_day_{start_date}_{end_date}_2200.csv\"\n",
    "df.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Data akumulasi harian {csv_filename} berhasil disimpan\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. KONVERSI CSV TO TIFF PER 3 JAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from scipy.spatial import cKDTree\n",
    "import os\n",
    "\n",
    "def idw_interpolation(x, y, z, xi, yi, power=2):\n",
    "    tree = cKDTree(np.array(list(zip(x, y))))\n",
    "    dist, idx = tree.query(np.array(list(zip(xi.ravel(), yi.ravel()))), k=10)\n",
    "    weights = 1 / dist**power\n",
    "    weights /= weights.sum(axis=1, keepdims=True)\n",
    "    zi = np.sum(z[idx] * weights, axis=1)\n",
    "    return zi.reshape(xi.shape)\n",
    "\n",
    "def interpolate_and_save_to_tiff(csv_file, output_dir):\n",
    "    # Membaca data dari CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Data koordinat dan nilai untuk interpolasi\n",
    "    x = df['longitude'].values\n",
    "    y = df['latitude'].values\n",
    "    z = df['prec'].values\n",
    "\n",
    "    # Tentukan bounding box dan resolusi grid\n",
    "    xmin, xmax = x.min(), x.max()\n",
    "    ymin, ymax = y.min(), y.max()\n",
    "    res = 0.092\n",
    "    grid_x, grid_y = np.meshgrid(np.arange(xmin, xmax + res, res), np.arange(ymin, ymax + res, res))\n",
    "\n",
    "    # Lakukan interpolasi IDW\n",
    "    grid_z = idw_interpolation(x, y, z, grid_x, grid_y)\n",
    "\n",
    "    # Simpan hasil interpolasi ke file GeoTIFF\n",
    "    file_name = os.path.basename(csv_file).replace('.csv', '.tif')\n",
    "    output_tiff = os.path.join(output_dir, file_name)\n",
    "    transform = from_origin(xmin, ymax, res, res)\n",
    "    crs = rasterio.crs.CRS.from_proj4(\"+proj=longlat +datum=WGS84 +no_defs\")\n",
    "\n",
    "    with rasterio.open(output_tiff, 'w', driver='GTiff', height=grid_z.shape[0], width=grid_z.shape[1],\n",
    "                       count=1, dtype=grid_z.dtype, crs=crs, transform=transform) as dst:\n",
    "        dst.write(grid_z, 1)\n",
    "\n",
    "    print(f\"Interpolasi disimpan di: {output_tiff}\")\n",
    "\n",
    "def process_csv_folder(input_dir, output_dir):\n",
    "    # Ambil semua file CSV di folder input\n",
    "    csv_files = [os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.csv')]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"Memproses file: {csv_file}\")\n",
    "        interpolate_and_save_to_tiff(csv_file, output_dir)\n",
    "\n",
    "# Contoh Penggunaan\n",
    "process_csv_folder(\n",
    "    input_dir=\"../repository/processing/hour\",\n",
    "    output_dir=\"../repository/post-processing/idw-masked/hour\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. KONVERSI CSV TO TIFF PER 3 JAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from scipy.spatial import cKDTree\n",
    "import os\n",
    "\n",
    "def idw_interpolation(x, y, z, xi, yi, power=2):\n",
    "    tree = cKDTree(np.array(list(zip(x, y))))\n",
    "    dist, idx = tree.query(np.array(list(zip(xi.ravel(), yi.ravel()))), k=10)\n",
    "    weights = 1 / dist**power\n",
    "    weights /= weights.sum(axis=1, keepdims=True)\n",
    "    zi = np.sum(z[idx] * weights, axis=1)\n",
    "    return zi.reshape(xi.shape)\n",
    "\n",
    "def interpolate_and_save_to_tiff(csv_file, output_dir):\n",
    "    # Membaca data dari CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Data koordinat dan nilai untuk interpolasi\n",
    "    x = df['longitude'].values\n",
    "    y = df['latitude'].values\n",
    "    z = df['prec'].values\n",
    "\n",
    "    # Tentukan bounding box dan resolusi grid\n",
    "    xmin, xmax = x.min(), x.max()\n",
    "    ymin, ymax = y.min(), y.max()\n",
    "    res = 0.092\n",
    "    grid_x, grid_y = np.meshgrid(np.arange(xmin, xmax + res, res), np.arange(ymin, ymax + res, res))\n",
    "\n",
    "    # Lakukan interpolasi IDW\n",
    "    grid_z = idw_interpolation(x, y, z, grid_x, grid_y)\n",
    "\n",
    "    # Simpan hasil interpolasi ke file GeoTIFF\n",
    "    file_name = os.path.basename(csv_file).replace('.csv', '.tif')\n",
    "    output_tiff = os.path.join(output_dir, file_name)\n",
    "    transform = from_origin(xmin, ymax, res, res)\n",
    "    crs = rasterio.crs.CRS.from_proj4(\"+proj=longlat +datum=WGS84 +no_defs\")\n",
    "\n",
    "    with rasterio.open(output_tiff, 'w', driver='GTiff', height=grid_z.shape[0], width=grid_z.shape[1],\n",
    "                       count=1, dtype=grid_z.dtype, crs=crs, transform=transform) as dst:\n",
    "        dst.write(grid_z, 1)\n",
    "\n",
    "    print(f\"Interpolasi disimpan di: {output_tiff}\")\n",
    "\n",
    "def process_csv_folder(input_dir, output_dir):\n",
    "    # Ambil semua file CSV di folder input\n",
    "    csv_files = [os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.csv')]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"Memproses file: {csv_file}\")\n",
    "        interpolate_and_save_to_tiff(csv_file, output_dir)\n",
    "\n",
    "# Contoh Penggunaan\n",
    "process_csv_folder(\n",
    "    input_dir=\"../repository/processing/day/csv-to-idw\",\n",
    "    output_dir=\"../repository/post-processing/idw-masked/day\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direktori data\n",
    "import requests\n",
    "\n",
    "\n",
    "shp_dirs = [\"../repository/post-processing/csv-to-vektor/balai\", \"../repository/post-processing/csv-to-vektor/pulau\"]\n",
    "tif_dirs = [\"../repository/post-processing/idw-masked/day\", \"../repository/post-processing/idw-masked/hour\"]\n",
    "geoserver_endpoint = \"http://admin:geoserver@127.0.0.1:8080/geoserver\"\n",
    "workspace = \"demo_simadu\"\n",
    "\n",
    "def upload_to_geoserver(data_path, store_name, geoserver_endpoint, workspace):\n",
    "    file_extension = os.path.splitext(data_path)[1].lower()\n",
    "    if file_extension == \".shp\":\n",
    "        file_type = \"shp\"\n",
    "        store_type = \"datastores\"\n",
    "    elif file_extension == \".tif\":\n",
    "        file_type = \"geotiff\"\n",
    "        store_type = \"coveragestores\"\n",
    "    else:\n",
    "        print(f\"File type {file_extension} not supported for upload.\")\n",
    "        return False\n",
    "\n",
    "    absolute_path = os.path.abspath(data_path).replace(\"\\\\\", \"/\")\n",
    "    url = f\"{geoserver_endpoint}/rest/workspaces/{workspace}/{store_type}/{store_name}/external.{file_type}\"\n",
    "\n",
    "    headers = {\"Content-type\": \"text/plain\"}\n",
    "    response = requests.put(url, data=f\"file://{absolute_path}\", headers=headers, auth=(\"admin\", \"geoserver\"))\n",
    "\n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f\"Berhasil upload {data_path} ke geoserver\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Gagal upload {data_path} ke geoserver. Status code: {response.status_code}\")\n",
    "        return False\n",
    "\n",
    "def process_and_upload_to_geoserver(shp_dirs, tif_dirs, geoserver_endpoint, workspace):\n",
    "    for shp_dir in shp_dirs:\n",
    "        shp_files = [os.path.join(shp_dir, file) for file in os.listdir(shp_dir) if file.endswith('.shp')]\n",
    "        for shp_file in shp_files:\n",
    "            store_name = os.path.splitext(os.path.basename(shp_file))[0]\n",
    "            upload_to_geoserver(shp_file, store_name, geoserver_endpoint, workspace)\n",
    "\n",
    "    for tif_dir in tif_dirs:\n",
    "        tif_files = [os.path.join(tif_dir, file) for file in os.listdir(tif_dir) if file.endswith('.tif')]\n",
    "        for tif_file in tif_files:\n",
    "            store_name = os.path.splitext(os.path.basename(tif_file))[0]\n",
    "            upload_to_geoserver(tif_file, store_name, geoserver_endpoint, workspace)\n",
    "\n",
    "# Jalankan proses\n",
    "process_and_upload_to_geoserver(\n",
    "    shp_dirs,\n",
    "    tif_dirs,\n",
    "    geoserver_endpoint,\n",
    "    workspace,\n",
    ")\n",
    "# opacity=\"0\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
