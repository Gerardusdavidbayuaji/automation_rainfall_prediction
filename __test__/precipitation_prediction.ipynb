{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ftplib\n",
    "import datetime\n",
    "import requests\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rasterio.mask import mask\n",
    "from scipy.spatial import cKDTree\n",
    "from shapely.geometry import Point\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from rasterio.transform import from_origin\n",
    "from netCDF4 import Dataset, date2num, num2date\n",
    "# from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_nc_file = \"../data/row\"\n",
    "path_nc_row = \"../repository/pre-processing/row\"\n",
    "path_modified = \"../repository/pre-processing/result-row\"\n",
    "path_pch_tabular = \"../data/tabular/data_pch_balai.xlsx\"\n",
    "mask_pulau = \"../data/geojson/pulau.geojson\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftp_host = os.getenv(\"HOST\")\n",
    "ftp_user = os.getenv(\"USER\")\n",
    "ftp_password = os.getenv(\"PASSWORD\")\n",
    "cycle = \"12\"\n",
    "\n",
    "def connect_ftp():\n",
    "    ftp = ftplib.FTP(ftp_host)\n",
    "    ftp.login(ftp_user, ftp_password)\n",
    "    ftp.cwd(\"/\")\n",
    "    return ftp\n",
    "\n",
    "def download_file_from_ftp(ftp, filename):\n",
    "    try:\n",
    "        file_list = ftp.nlst()\n",
    "        if filename in file_list:\n",
    "            local_file_path = os.path.join(path_nc_file, filename)\n",
    "            if not os.path.exists(local_file_path):\n",
    "                with open(local_file_path, \"wb\") as local_file:\n",
    "                    ftp.retrbinary(f\"RETR {filename}\", local_file.write)\n",
    "                print(f\"Download successfully {filename}\")\n",
    "            else:\n",
    "                print(f\"File {filename} is available\")\n",
    "            return local_file_path\n",
    "    except Exception:\n",
    "        print(\"File is corrupted, and there is nothing that can be done.\")\n",
    "        return None\n",
    "\n",
    "def download_latest_file_from_ftp(ftp):\n",
    "    file_list = ftp.nlst()\n",
    "    if file_list:\n",
    "        latest_file = sorted(file_list)[-1]\n",
    "        return download_file_from_ftp(ftp, latest_file)\n",
    "    return None\n",
    "\n",
    "today = datetime.date.today() - datetime.timedelta(days=1)\n",
    "filename = f\"ECMWF.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "print(\"Downloading:\", filename)\n",
    "\n",
    "ftp = connect_ftp()\n",
    "if ftp:\n",
    "    local_file_path = download_file_from_ftp(ftp, filename) or download_latest_file_from_ftp(ftp)\n",
    "    ftp.quit()\n",
    "else:\n",
    "    print(\"Cannot connect to FTP server\")\n",
    "\n",
    "if local_file_path is None:\n",
    "    print(\"File is currently unavailable for download.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baca data .nc\n",
    "data = Dataset(local_file_path)\n",
    "\n",
    "lat = data.variables['lat'][:]\n",
    "lon = data.variables['lon'][:]\n",
    "prec = data.variables['tp'][:,0,:,:]\n",
    "time = data.variables['time'][:]\n",
    "\n",
    "dates = num2date(time, data.variables['time'].units)\n",
    "print((dates[7]+datetime.timedelta(hours=7)).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "print((dates[15]+datetime.timedelta(hours=7)).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "\n",
    "xrain = prec\n",
    "print(np.array_equal(xrain,prec))\n",
    "\n",
    "#change rainfall data\n",
    "for time in range (len(dates)):\n",
    "    for lati in range(len(lat)) :\n",
    "        for loni in range (len(lon)) :\n",
    "            if (time<=0) :\n",
    "                if (xrain[time,lati,loni]<=0) :\n",
    "                    xrain[time,lati,loni] == 0\n",
    "            elif(time>0) :\n",
    "                if (xrain[time,lati,loni]<0) :\n",
    "                    xrain[time,lati,loni] = xrain[time-1,lati,loni]\n",
    "                if (xrain[time,lati,loni]-xrain[time-1,lati,loni]<0) :\n",
    "                    xrain[time,lati,loni] = xrain[time-1,lati,loni]\n",
    "\n",
    "# initiate rain before changing rainfall accumulate to interval rainfall\n",
    "hjn = np.empty((len(dates),len(lat),len(lon)))\n",
    "# change rainfall accumulate to interval rainfall\n",
    "hjn[0,:,:] = xrain[0,:,:]\n",
    "for i in range (1,len(dates)) :\n",
    "    hjn[i,:,:] = xrain[i,:,:]-xrain[i-1,:,:]\n",
    "\n",
    "# reshape 3d to 4d so it can be sabed into netcdf permanent dimension\n",
    "hjn2 = hjn.reshape(len(dates), 1, len(lat), len(lon))\n",
    "\n",
    "# rewrite to netcdf\n",
    "ds = xr.open_dataset(local_file_path)\n",
    "ds['tp'].values = hjn2\n",
    "ds = ds.assign_coords(time=(\"time\",ds['time'].values + np.timedelta64(7,'h')))\n",
    "\n",
    "# output rewrite to netcdf\n",
    "output_rewrite = f\"ECMWF_new.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "output_path = os.path.join(path_nc_row, output_rewrite)\n",
    "ds.to_netcdf(output_path)\n",
    "print (ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_file_name = f\"ECMWF_new_3d.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "\n",
    "# # Gabungkan path dengan nama file\n",
    "# file_path = os.path.join(path_modified, result_file_name)\n",
    "\n",
    "# # Buat file NetCDF baru\n",
    "# f = Dataset(file_path, 'w', format='NETCDF4')\n",
    "# print(f\"File {result_file_name} berhasil dibuat\")\n",
    "# print (data.variables)\n",
    "\n",
    "# # define variables foe new netcdf4 file\n",
    "# rain = hjn[:48,:,:]\n",
    "# latitude = data.variables[\"lat\"][:]\n",
    "# longitude = data.variables[\"lon\"][:]\n",
    "# time_k = data.variables['time'][:48]\n",
    "\n",
    "# tempgrp = f.createGroup('Rain_data')\n",
    "\n",
    "# # Create dimension for netCDF4\n",
    "# f.createDimension('lon', len(longitude))\n",
    "# f.createDimension('lat', len(latitude))\n",
    "# f.createDimension('time', len(time_k))\n",
    "\n",
    "# # Create variables for netCDF4\n",
    "# lon = f.createVariable('lon', 'f4', 'lon')\n",
    "# lat = f.createVariable('lat', 'f4', 'lat')  \n",
    "# rain = f.createVariable('rain', 'f4', ('time', 'lat', 'lon'))\n",
    "# time = f.createVariable('time', 'i4', 'time')\n",
    "\n",
    "# # define variables to be saved into netcdf4 file\n",
    "# lon[:] = longitude[:] #The \"[:]\" at the end of the variable instance is necessary\n",
    "# lat[:] = latitude[:]\n",
    "# rain[:,:,:] = hjn[:48,:,:]\n",
    "# time[:] = time_k+7\n",
    "\n",
    "# print (dates[0].strftime('%Y-%m-%d ')+str(cycle)+\":00:00\")\n",
    "\n",
    "# #Add global attributes\n",
    "# f.description = \"ECMWF from BMKG modified by Jhon doe\"\n",
    "# f.history = \"Created \" + today.strftime(\"%d/%m/%y\")\n",
    "\n",
    "# #Add local attributes to variable instances\n",
    "# lon.units = 'degree_east'\n",
    "# lat.units = 'degree_north'\n",
    "# time.units = 'hours since '+(dates[0]+datetime.timedelta(hours=7)).strftime('%Y-%m-%d ')+str(cycle)+\":00:00\"\n",
    "# rain.units = 'mm'\n",
    "\n",
    "# # close file\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"../repository/pre-processing/result-row\\ECMWF_new_3d.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "\n",
    "if (cycle == '00') :\n",
    "    n = 4\n",
    "else :\n",
    "    n = 0\n",
    "\n",
    "data = Dataset(data_path)\n",
    "print(data.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROSES UNTUK WMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check waktu data\n",
    "time = data.variables['time'][:]\n",
    "dates = num2date(time, data.variables['time'].units)\n",
    "print((dates[10+n]).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "print((dates[16+n]).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "\n",
    "lon_wms = np.array(data.variables[\"lon\"][:])\n",
    "lat_wms = np.array(data.variables[\"lat\"][:])\n",
    "prec_wms = np.array(data.variables[\"rain\"][:])\n",
    "\n",
    "# Inisialisasi akumulasi, meshgrid, tanggal\n",
    "xx, yy = np.meshgrid(lon, lat)\n",
    "accumulation = 0\n",
    "start_date = dates[11+n - 1].strftime('%m%d%Y')\n",
    "\n",
    "input_hour_dir = f\"../repository/post-processing/nc_to_tiff/pch_hour_{start_date}/\"\n",
    "input_day_dir = f\"../repository/post-processing/nc_to_tiff/pch_day_{start_date}/\"\n",
    "output_masked_hour_dir = f\"../repository/post-processing/wms/hour/pch_hour_{start_date}/\"\n",
    "output_masked_day_dir = f\"../repository/post-processing/wms/day/pch_day_{start_date}/\"\n",
    "mask_pulau = \"../data/geojson/pulau.geojson\"\n",
    "\n",
    "os.makedirs(input_hour_dir, exist_ok=True)\n",
    "os.makedirs(input_day_dir, exist_ok=True)\n",
    "os.makedirs(output_masked_hour_dir, exist_ok=True)\n",
    "os.makedirs(output_masked_day_dir, exist_ok=True)\n",
    "\n",
    "for k in range(11+n, 19+n):\n",
    "    hour_data = prec[k, :, :]\n",
    "    accumulation += hour_data\n",
    "\n",
    "    start_hour = dates[k - 1].strftime('%H%M')\n",
    "\n",
    "    resolution_lon = (lon_wms.max() - lon_wms.min()) / lon_wms.shape[0]\n",
    "    resolution_lat = (lat_wms.max() - lat_wms.min()) / lat_wms.shape[0]\n",
    "    transform = from_origin(lon_wms.min(), lat_wms.max(), resolution_lon, resolution_lat)\n",
    "\n",
    "    tiff_filename_hour = os.path.join(input_hour_dir, f\"pch_hour_{start_date}_{start_hour}.tif\")\n",
    "\n",
    "    with rasterio.open(\n",
    "        tiff_filename_hour,\n",
    "        'w',\n",
    "        driver='GTiff',\n",
    "        height=hour_data.shape[0],\n",
    "        width=hour_data.shape[1],\n",
    "        count=1,\n",
    "        dtype=hour_data.dtype,\n",
    "        crs=rasterio.crs.CRS.from_proj4(\"+proj=longlat +datum=WGS84 +no_defs\"),\n",
    "        transform=transform\n",
    "    ) as dst:\n",
    "        dst.write(hour_data, 1)\n",
    "\n",
    "    print(f\"Successfully convert 3H netCDF to tiff: {tiff_filename_hour}\")\n",
    "\n",
    "tiff_filename_day = os.path.join(input_day_dir, f\"pch_day_{start_date}_2200.tif\")\n",
    "\n",
    "with rasterio.open(\n",
    "    tiff_filename_day,\n",
    "    'w',\n",
    "    driver='GTiff',\n",
    "    height=accumulation.shape[0],\n",
    "    width=accumulation.shape[1],\n",
    "    count=1,\n",
    "    dtype=accumulation.dtype,\n",
    "    crs=rasterio.crs.CRS.from_proj4(\"+proj=longlat +datum=WGS84 +no_defs\"),\n",
    "    transform=transform\n",
    ") as dst:\n",
    "    dst.write(accumulation, 1)\n",
    "\n",
    "print(f\"Successfully convert 1D netCDF to tiff: {tiff_filename_day}\")\n",
    "\n",
    "def masked_data(input_tiff, mask_file, output_masked_dir):\n",
    "    gdf = gpd.read_file(mask_file)\n",
    "    geometries = [geom for geom in gdf.geometry]\n",
    "\n",
    "    with rasterio.open(input_tiff) as src:\n",
    "        out_image, out_transform = mask(src, geometries, crop=True)\n",
    "        out_meta = src.meta.copy()\n",
    "\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": out_image.shape[1],\n",
    "        \"width\": out_image.shape[2],\n",
    "        \"transform\": out_transform\n",
    "    })\n",
    "\n",
    "    file_name = os.path.basename(input_tiff)\n",
    "    output_masked_tiff = os.path.join(output_masked_dir, file_name)\n",
    "\n",
    "    with rasterio.open(output_masked_tiff, \"w\", **out_meta) as dst:\n",
    "        dst.write(out_image)\n",
    "\n",
    "    print(f\"Result masked: {output_masked_tiff}\")\n",
    "\n",
    "for file in os.listdir(input_hour_dir):\n",
    "    if file.endswith(\".tif\"):\n",
    "        input_tiff = os.path.join(input_hour_dir, file)\n",
    "        masked_data(input_tiff, mask_pulau, output_masked_hour_dir)\n",
    "\n",
    "for file in os.listdir(input_day_dir):\n",
    "    if file.endswith(\".tif\"):\n",
    "        input_tiff = os.path.join(input_day_dir, file)\n",
    "        masked_data(input_tiff, mask_pulau, output_masked_day_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROSES UNTUK WFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check waktu data\n",
    "time = data.variables['time'][:]\n",
    "dates = num2date(time, data.variables['time'].units)\n",
    "print((dates[10+n]).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "print((dates[16+n]).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "\n",
    "lat_wfs = data.variables[\"lat\"][:]\n",
    "lon_wfs = data.variables[\"lon\"][:]\n",
    "prec_wfs = data.variables['rain'][:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max.columns\",None)\n",
    "grid = pd.read_excel(path_pch_tabular)\n",
    "print (grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Data index lon, lat, balai, ws, kota\n",
    "grid_long = grid['idx_long'].to_numpy()\n",
    "grid_lat = grid['idx_lat'].to_numpy()\n",
    "longitude_r = grid['long_data']\n",
    "latitude_r = grid['lat_data']\n",
    "latitude_prod = grid['lat_prod']\n",
    "longitude_prod = grid['long_prod']\n",
    "pulau = grid['pulau']\n",
    "balai = grid['balai']\n",
    "kode_balai = grid['kode_balai']\n",
    "ws = grid ['wilayah_sungai']\n",
    "das = grid['das']\n",
    "prov= grid[\"provinsi\"]\n",
    "kota = grid['kabkot']\n",
    "wilayah = grid['wilayah']\n",
    "latshape = grid_lat.shape[0]\n",
    "latshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting 1 day ahead\n",
    "for k in range (11+n,19+n):\n",
    "    print((dates[k]).strftime(\"%Y%m%d%H\"))\n",
    "    idx_t=(dates[k]).strftime(\"%Y%m%d%H\")\n",
    "    if (k==19+n):\n",
    "        globals()['hujanharian_'+(idx_t)] = prec_wfs[11+n,:,:]\n",
    "    else:\n",
    "        globals()['hujanharian_'+(idx_t)] = prec_wfs[11+n:k+1,:,:].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasting 2 days ahead\n",
    "for k in range (11+8+n,19+8+n):\n",
    "    print((dates[k]).strftime(\"%Y%m%d%H\"))\n",
    "    idx_t=(dates[k]).strftime(\"%Y%m%d%H\")\n",
    "    if (k==19+8+n):\n",
    "        globals()['hujanharian_'+(idx_t)] = prec_wfs[11+8+n,:,:]\n",
    "    else:\n",
    "        globals()['hujanharian_'+(idx_t)] = prec_wfs[11+8+n:k+1,:,:].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kolom = ['long_prod', 'lat_prod', 'tanggal', 'longitude','latitude','pulau', 'kode_balai', 'balai','das','provinsi','kabkot','wilayah']\n",
    "df_dasWaspada = pd.DataFrame(columns=kolom)\n",
    "for tab in range (latshape) :\n",
    "    gridlat = grid_lat[tab]\n",
    "    gridlon = grid_long[tab]\n",
    "    for k in range (11+n,27+n):\n",
    "        #utk cek awal mulai waspada\n",
    "        idx_t = (dates[k]).strftime(\"%Y%m%d%H\")\n",
    "        idx_h = (dates[k]).strftime(\"%H:00\")\n",
    "        hujan_cek = globals()['hujanharian_'+(idx_t)]\n",
    "        \n",
    "        #untuk tanggal status siaga banjir dan pengecekan status akhir siaga banjir di tiap grid\n",
    "        i_idx = 11+n if k<19+n else 19+n\n",
    "        tanggal = (dates[i_idx]).strftime(\"%d %B %Y\")\n",
    "        \n",
    "        if (hujan_cek[gridlat,gridlon]>=0.5):\n",
    "            df = pd.DataFrame([{'tanggal':tanggal, 'long_prod':longitude_prod[tab], 'lat_prod':latitude_prod[tab], 'longitude':longitude_r[tab],'latitude':latitude_r[tab], 'pulau':pulau[tab], 'kode_balai':kode_balai[tab], 'balai':balai[tab],\\\n",
    "                             'das':das[tab],'provinsi':prov[tab],'kabkot':kota[tab],'wilayah':wilayah[tab]\\\n",
    "                              ,'waktu_mulai':idx_h}])\n",
    "            i_idx = (11+n) if k<(19+n) else (19+n)\n",
    "            for i in range (i_idx,i_idx+8):\n",
    "                idx_t = (dates[i]).strftime(\"%Y%m%d%H\")\n",
    "                idx_h = (dates[i]).strftime(\"%H:00\")\n",
    "                df['ch_'+idx_h] = globals()['hujanharian_'+(idx_t)][gridlat,gridlon]\n",
    "            \n",
    "            kelas=globals()['hujanharian_'+(idx_t)][gridlat,gridlon]\n",
    "            if (0.5<kelas<=20):\n",
    "                status=\"1\" #HUJAN RINGAN\n",
    "            elif(20<kelas<=50):\n",
    "                status=\"2\" #HUJAN SEDANG\n",
    "            elif(50<kelas<=100):\n",
    "                status=\"3\" #HUJAN LEBAT\n",
    "            elif(100<kelas<=150):\n",
    "                status=\"4\" #HUJAN SANGAT LEBAT\n",
    "            elif(kelas>150):\n",
    "                status=\"5\" #HUJAN EKSTREM\n",
    "                \n",
    "            df[\"klasifikasi_hujan\"] = status\n",
    "            \n",
    "            status_cek = globals()['hujanharian_'+(idx_t)][gridlat,gridlon]\n",
    "            if (0.5<status_cek<=50):\n",
    "                status_1=\"1\" #AMAN\n",
    "            elif(50<status_cek<=75):\n",
    "                status_1=\"2\" #WASPADA\n",
    "            elif(75<status_cek<=100):\n",
    "                status_1=\"3\" #SIAGA\n",
    "            elif(status_cek>100):\n",
    "                status_1=\"4\" #AWAS\n",
    "            \n",
    "            df[\"status_akhir\"] = status_1\n",
    "            \n",
    "            df_dasWaspada = pd.concat([df_dasWaspada,df])\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "df = df_dasWaspada.sort_values(by=\"tanggal\")\n",
    "df = df.set_index(\"tanggal\")\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('../repository/pre-processing/accumulation/accum_'+\\\n",
    "                        (dates[11+n]).strftime('%m%d%Y')+'_' #%Y%m%d\n",
    "                        +(dates[26+n]).strftime('%m%d%Y')+'.xlsx', engine='xlsxwriter')\n",
    "# Write each dataframe to a different worksheet.\n",
    "df.to_excel(writer, sheet_name='Akumulasi Berjalan')\n",
    "writer.close()\n",
    "\n",
    "print('Done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_excel(\"../new-repository/pre-processing/accumulation/accum_01172025_01192025.xlsx\")\n",
    "data = pd.read_excel(\n",
    "    f\"../repository/pre-processing/accumulation/accum_{(dates[11+n]).strftime('%m%d%Y')}_{(dates[26+n]).strftime('%m%d%Y')}.xlsx\"\n",
    ")\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "tanggal_hari_ini = datetime.now()\n",
    "tanggal_besok = tanggal_hari_ini + timedelta(days=1)\n",
    "\n",
    "# Formatkan tanggal besok ke string\n",
    "tanggal_besok_str = tanggal_besok.strftime(\"%d %B %Y\")\n",
    "print(\"Tanggal besok:\", tanggal_besok_str)\n",
    "\n",
    "data_tanggal_besok = data[data['tanggal'] == tanggal_besok_str]\n",
    "data_tanggal_besok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tanggal_besok_sorted = data_tanggal_besok.sort_values(by='ch_01:00', ascending=False)\n",
    "data_tanggal_besok_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_remove_balai = data_tanggal_besok_sorted.drop_duplicates(subset=['balai', 'kabkot'], keep='first')\n",
    "print('cek data tabular', data_remove_balai)\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('../repository/pre-processing/remove-duplicate/balai/remdup_balai_'+\\\n",
    "                        (dates[11+n]).strftime('%m%d%Y')+'.xlsx', engine='xlsxwriter')\n",
    "# Write each dataframe to a different worksheet.\n",
    "data_remove_balai.to_excel(writer, sheet_name='Balai')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.close()\n",
    "\n",
    "print('Done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = pd.pivot_table(\n",
    "    data_remove_balai,\n",
    "    index='balai',\n",
    "    columns='klasifikasi_hujan',\n",
    "    values='kabkot',\n",
    "    aggfunc='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Hasil\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_remove_kab = data_tanggal_besok_sorted.drop_duplicates(subset=['kabkot'], keep='first')\n",
    "#Ini yang untuk grafik perpulau, tinggal diklasifikasikan saja berdasarkan pulau. begitu juga untuk tabel kasifikasi berdasarkan wilayah\n",
    "print('cek data tabular',data_remove_kab)\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('../repository/pre-processing/remove-duplicate/pulau/remdup_pulau_'+\\\n",
    "                        (dates[11+n]).strftime('%m%d%Y')+'.xlsx', engine='xlsxwriter')\n",
    "# Write each dataframe to a different worksheet.\n",
    "data_remove_kab.to_excel(writer, sheet_name='Pulau')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.close()\n",
    "\n",
    "print('Done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = pd.pivot_table(\n",
    "    data_remove_kab,\n",
    "    index='pulau',       # Baris pivot table\n",
    "    columns='klasifikasi_hujan',    # Kolom pivot table\n",
    "    values='kabkot',     # Kolom yang akan dihitung\n",
    "    aggfunc='count',        # Fungsi agregasi\n",
    "    fill_value=0            # Mengisi nilai NaN dengan 0\n",
    ")\n",
    "\n",
    "# Hasil\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balai_remdup_path = '../repository/pre-processing/remove-duplicate/balai/remdup_balai_'+(dates[11+n]).strftime('%m%d%Y')+'.xlsx'\n",
    "data = pd.read_excel(balai_remdup_path)\n",
    "\n",
    "get_columns_from_balai_remdup = [\n",
    "    'tanggal', 'long_prod', 'lat_prod', 'longitude', 'latitude', 'wilayah', 'kode_balai', 'balai', 'ch_01:00', 'klasifikasi_hujan', 'status_akhir'\n",
    "]\n",
    "\n",
    "data_filtered_by_balai_remdup = data[get_columns_from_balai_remdup]\n",
    "\n",
    "add_columns = [\n",
    "    'total_kl_1', 'total_kl_2', 'total_kl_3', 'total_kl_4', 'total_kl_5',\n",
    "    'total_kg_1', 'total_kg_2', 'total_kg_3', 'total_kg_4',\n",
    "    'kelas_kl_1', 'kelas_kl_2', 'kelas_kl_3', 'kelas_kl_4', 'kelas_kl_5',\n",
    "    'kelas_kg_1', 'kelas_kg_2', 'kelas_kg_3', 'kelas_kg_4',\n",
    "    'last_data', 'last_updt'\n",
    "]\n",
    "\n",
    "for col in add_columns:\n",
    "    data_filtered_by_balai_remdup[col] = 0\n",
    "\n",
    "# menghitung jumlah total_kl_1 sampai total_kl_5 per wilayah\n",
    "for wilayah in data_filtered_by_balai_remdup['wilayah'].unique():\n",
    "    wilayah_data = data_filtered_by_balai_remdup[data_filtered_by_balai_remdup['wilayah'] == wilayah]\n",
    "    for i in range (1, 6): \n",
    "        data_filtered_by_balai_remdup.loc[data_filtered_by_balai_remdup['wilayah'] == wilayah, f'total_kl_{i}'] = wilayah_data[wilayah_data['klasifikasi_hujan'] == i].shape[0]\n",
    "\n",
    "# menghitung jumlah total_kg_1 sampai total_kg_4 per wilayah\n",
    "for wilayah in data_filtered_by_balai_remdup['wilayah'].unique():\n",
    "    wilayah_data = data_filtered_by_balai_remdup[data_filtered_by_balai_remdup['wilayah'] == wilayah]\n",
    "    for i in range (1, 5): \n",
    "        data_filtered_by_balai_remdup.loc[data_filtered_by_balai_remdup['wilayah'] == wilayah, f'total_kg_{i}'] = wilayah_data[wilayah_data['status_akhir'] == i].shape[0]\n",
    "\n",
    "# menghitung jumlah kelas_kl_1 sampai kelas_kl_5 per balai\n",
    "for balai in data_filtered_by_balai_remdup['balai'].unique():\n",
    "    balai_data = data_filtered_by_balai_remdup[data_filtered_by_balai_remdup['balai'] == balai]\n",
    "    for i in range (1, 6): \n",
    "        data_filtered_by_balai_remdup.loc[data_filtered_by_balai_remdup['balai'] == balai, f'kelas_kl_{i}'] = balai_data[balai_data['klasifikasi_hujan'] == i].shape[0]\n",
    "\n",
    "# menghitung jumlah kelas_kg_1 sampai kelas_kg_4 per balai\n",
    "for balai in data_filtered_by_balai_remdup['balai'].unique():\n",
    "    balai_data = data_filtered_by_balai_remdup[data_filtered_by_balai_remdup['balai'] == balai]\n",
    "    for i in range (1, 5): \n",
    "        data_filtered_by_balai_remdup.loc[data_filtered_by_balai_remdup['balai'] == balai, f'kelas_kg_{i}'] = balai_data[balai_data['status_akhir'] == i].shape[0]\n",
    "\n",
    "data_filtered_by_balai_remdup['last_data'] = filename\n",
    "data_filtered_by_balai_remdup['last_updt'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "data_filtered_by_balai_remdup = data_filtered_by_balai_remdup.drop_duplicates(subset=['balai'])        \n",
    "\n",
    "output_balai_day = '../repository/processing/day/balai/balai_pch_day_'+(dates[11+n]).strftime('%m%d%Y')+'_2200.csv'\n",
    "data_filtered_by_balai_remdup.to_csv(output_balai_day, index=False)\n",
    "\n",
    "print('done...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pulau_remdup_path = '../repository/pre-processing/remove-duplicate/pulau/remdup_pulau_'+(dates[11+n]).strftime('%m%d%Y')+'.xlsx'\n",
    "\n",
    "data = pd.read_excel(pulau_remdup_path)\n",
    "\n",
    "get_columns_from_remdup = [\n",
    "    'tanggal', 'long_prod', 'lat_prod', 'longitude', 'latitude', 'wilayah', 'pulau', 'ch_01:00', 'klasifikasi_hujan', 'status_akhir'\n",
    "]\n",
    "\n",
    "data_filtered_by_pulau_remdup = data[get_columns_from_remdup]\n",
    "# print(get_columns_from_remdup)\n",
    "\n",
    "add_columns = [\n",
    "    'total_kl_1', 'total_kl_2', 'total_kl_3', 'total_kl_4', 'total_kl_5',\n",
    "    'total_kg_1', 'total_kg_2', 'total_kg_3', 'total_kg_4',\n",
    "    'kelas_kl_1', 'kelas_kl_2', 'kelas_kl_3', 'kelas_kl_4', 'kelas_kl_5',\n",
    "    'kelas_kg_1', 'kelas_kg_2', 'kelas_kg_3', 'kelas_kg_4',\n",
    "    'last_data', 'last_updt'\n",
    "]\n",
    "\n",
    "for col in add_columns:\n",
    "    data_filtered_by_pulau_remdup[col] = 0\n",
    "\n",
    "# menghitung jumlah total_kl_1 sampai total_kl_5 per wilayah\n",
    "for wilayah in data_filtered_by_pulau_remdup['wilayah'].unique():\n",
    "    wilayah_data = data_filtered_by_pulau_remdup[data_filtered_by_pulau_remdup['wilayah'] == wilayah]\n",
    "    for i in range (1, 6): \n",
    "        data_filtered_by_pulau_remdup.loc[data_filtered_by_pulau_remdup['wilayah'] == wilayah, f'total_kl_{i}'] = wilayah_data[wilayah_data['klasifikasi_hujan'] == i].shape[0]\n",
    "\n",
    "# menghitung jumlah total_kg_1 sampai total_kg_4 per wilayah\n",
    "for wilayah in data_filtered_by_pulau_remdup['wilayah'].unique():\n",
    "    wilayah_data = data_filtered_by_pulau_remdup[data_filtered_by_pulau_remdup['wilayah'] == wilayah]\n",
    "    for i in range (1, 5): \n",
    "        data_filtered_by_pulau_remdup.loc[data_filtered_by_pulau_remdup['wilayah'] == wilayah, f'total_kg_{i}'] = wilayah_data[wilayah_data['status_akhir'] == i].shape[0]\n",
    "\n",
    "# menghitung jumlah kelas_kl_1 sampai kelas_kl_5 per pulau\n",
    "for pulau in data_filtered_by_pulau_remdup['pulau'].unique():\n",
    "    pulau_data = data_filtered_by_pulau_remdup[data_filtered_by_pulau_remdup['pulau'] == pulau]\n",
    "    for i in range (1, 6): \n",
    "        data_filtered_by_pulau_remdup.loc[data_filtered_by_pulau_remdup['pulau'] == pulau, f'kelas_kl_{i}'] = pulau_data[pulau_data['klasifikasi_hujan'] == i].shape[0]\n",
    "\n",
    "# menghitung jumlah kelas_kg_1 sampai kelas_kg_4 per pulau\n",
    "for pulau in data_filtered_by_pulau_remdup['pulau'].unique():\n",
    "    pulau_data = data_filtered_by_pulau_remdup[data_filtered_by_pulau_remdup['pulau'] == pulau]\n",
    "    for i in range (1, 5): \n",
    "        data_filtered_by_pulau_remdup.loc[data_filtered_by_pulau_remdup['pulau'] == pulau, f'kelas_kg_{i}'] = pulau_data[pulau_data['status_akhir'] == i].shape[0]\n",
    "\n",
    "data_filtered_by_pulau_remdup['last_data'] = filename\n",
    "data_filtered_by_pulau_remdup['last_updt'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "data_filtered_by_pulau_remdup = data_filtered_by_pulau_remdup.drop_duplicates(subset=['pulau'])        \n",
    "\n",
    "output_pulau_day = '../repository/processing/day/pulau/pulau_pch_day_'+(dates[11+n]).strftime('%m%d%Y')+'_2200.csv'\n",
    "data_filtered_by_pulau_remdup.to_csv(output_pulau_day, index=False)\n",
    "\n",
    "print('done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_shp(csv_file, output_shp):\n",
    "    data = pd.read_csv(csv_file)\n",
    "\n",
    "    columns_to_drop = ['ch_01:00', 'klasifikasi_hujan', 'status_akhir']\n",
    "    data = data.drop(columns=[col for col in columns_to_drop if col in data.columns], errors='ignore')\n",
    "\n",
    "    if 'latitude' in data.columns and 'longitude' in data.columns:\n",
    "        data['geometry'] = data.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)\n",
    "\n",
    "        gdf = gpd.GeoDataFrame(data, geometry='geometry')\n",
    "        gdf.set_crs(epsg=4326, inplace=True)\n",
    "        gdf.to_file(output_shp, driver='ESRI Shapefile')\n",
    "\n",
    "        print(f\"Data vektor berhasil dibuat: {output_shp}\")\n",
    "    else:\n",
    "        print(\"Kolom 'latitude' atau 'longitude' tidak ditemukan dalam file CSV.\")\n",
    "\n",
    "csv_to_shp(\n",
    "    csv_file='../repository/processing/day/balai/balai_pch_day_'+(dates[11+n]).strftime('%m%d%Y')+'_2200.csv',\n",
    "    output_shp='../repository/post-processing/csv-to-vektor/balai/balai_pch_day_'+(dates[11+n]).strftime('%m%d%Y')+'_2200.shp'\n",
    ")\n",
    "\n",
    "csv_to_shp(\n",
    "    csv_file='../repository/processing/day/pulau/pulau_pch_day_'+(dates[11+n]).strftime('%m%d%Y')+'_2200.csv',\n",
    "    output_shp='../repository/post-processing/csv-to-vektor/pulau/pulau_pch_day_'+(dates[11+n]).strftime('%m%d%Y')+'_2200.shp'\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
