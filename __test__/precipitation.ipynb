{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ftplib\n",
    "import datetime\n",
    "import requests\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rasterio.mask import mask\n",
    "from scipy.spatial import cKDTree\n",
    "from shapely.geometry import Point\n",
    "# from datetime import datetime, timedelta\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from rasterio.transform import from_origin\n",
    "from netCDF4 import Dataset, date2num, num2date\n",
    "\n",
    "os.environ[\"PROJ_LIB\"] = \"C:/Users/2ndba/anaconda3/Library/share/proj\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPARATION\n",
    "Obtain the data and save it in this folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_nc_file = \"../data/row\"\n",
    "path_nc_row = \"../repository/pre-processing/row\"\n",
    "path_modified = \"../repository/pre-processing/result-row\"\n",
    "\n",
    "pch_tabular = \"../data/tabular/data_pch_balai.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RETRIEVE NETCDF DATA FROM THE FTP SERVER<br>\n",
    "\n",
    "<p>\n",
    "Obtain data for day -1 to use for tomorrow's predictions.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: ECMWF.0125.202501261200.PREC.nc\n",
      "Download successfully ECMWF.0125.202501261200.PREC.nc\n"
     ]
    }
   ],
   "source": [
    "# konfigurasi ftp\n",
    "ftp_host = os.getenv(\"HOST\")\n",
    "ftp_user = os.getenv(\"USER\")\n",
    "ftp_password = os.getenv(\"PASSWORD\")\n",
    "cycle = \"12\"\n",
    "\n",
    "# FTP functions\n",
    "def connect_ftp():\n",
    "    ftp = ftplib.FTP(ftp_host)\n",
    "    ftp.login(ftp_user, ftp_password)\n",
    "    ftp.cwd(\"/\")\n",
    "    return ftp\n",
    "\n",
    "def download_file_from_ftp(ftp, filename):\n",
    "    try:\n",
    "        file_list = ftp.nlst()\n",
    "        if filename in file_list:\n",
    "            local_file_path = os.path.join(path_nc_file, filename)\n",
    "            if not os.path.exists(local_file_path):\n",
    "                with open(local_file_path, \"wb\") as local_file:\n",
    "                    ftp.retrbinary(f\"RETR {filename}\", local_file.write)\n",
    "                print(f\"Download successfully {filename}\")\n",
    "            else:\n",
    "                print(f\"File {filename} is available\")\n",
    "            return local_file_path\n",
    "    except Exception:\n",
    "        print(\"File is corrupted, and there is nothing that can be done.\")\n",
    "        return None\n",
    "\n",
    "def download_latest_file_from_ftp(ftp):\n",
    "    file_list = ftp.nlst()\n",
    "    if file_list:\n",
    "        latest_file = sorted(file_list)[-1]\n",
    "        return download_file_from_ftp(ftp, latest_file)\n",
    "    return None\n",
    "\n",
    "# Download file .nc\n",
    "today = datetime.date.today() - datetime.timedelta(days=2)\n",
    "filename = f\"ECMWF.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "print(\"Downloading:\", filename)\n",
    "\n",
    "ftp = connect_ftp()\n",
    "if ftp:\n",
    "    local_file_path = download_file_from_ftp(ftp, filename) or download_latest_file_from_ftp(ftp)\n",
    "    ftp.quit()\n",
    "else:\n",
    "    print(\"Cannot connect to FTP server\")\n",
    "\n",
    "if local_file_path is None:\n",
    "    print(\"File is currently unavailable for download.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ THE NETCDF DATA<br/>\n",
    "\n",
    "<p>\n",
    "Data in netCDF format that has been downloaded contains four main parameters, namely lat (latitude), which represent latitude coordinates, lon (longitude) as longitude coordinates, tp (total precipitation) which shows total rainfall, and time which records the time the data was taken\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "Gunakan data H+1 dan H+2 untuk prediksi curah hujan\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-27 16:00:00 WIB\n",
      "2025-01-28 16:00:00 WIB\n"
     ]
    }
   ],
   "source": [
    "# Baca data .nc\n",
    "data = Dataset(local_file_path)\n",
    "\n",
    "lat = data.variables['lat'][:]\n",
    "lon = data.variables['lon'][:]\n",
    "prec = data.variables['tp'][:,0,:,:]\n",
    "time = data.variables['time'][:]\n",
    "\n",
    "dates = num2date(time, data.variables['time'].units)\n",
    "print((dates[7]+datetime.timedelta(hours=7)).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "print((dates[15]+datetime.timedelta(hours=7)).strftime('%Y-%m-%d %H:%M:%S WIB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proses pengolahan data curah hujan dimulai dengan menginisialisasi data hujan sebelum mengubahnya dari format akumulasi menjadi format interval waktu. Selanjutnya, data curah hujan akumulasi diubah menjadi data interval dengan menghitung selisih antara waktu saat ini dan waktu sebelumnya. Setelah itu, data yang berbentuk 3D (waktu, lintang, bujur) diubah menjadi format 4D dengan menambahkan dimensi tambahan agar sesuai dengan format permanen NetCDF. Akhirnya, data hasil olahan tersebut ditulis ulang ke dalam file NetCDF untuk keperluan penyimpanan dan analisis lebih lanjut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "<xarray.Dataset> Size: 43MB\n",
      "Dimensions:  (lat: 185, lon: 449, lev: 1, time: 65)\n",
      "Coordinates:\n",
      "  * lat      (lat) float64 1kB 9.0 8.875 8.75 8.625 ... -13.75 -13.88 -14.0\n",
      "  * lon      (lon) float64 4kB 92.0 92.12 92.25 92.38 ... 147.8 147.9 148.0\n",
      "  * lev      (lev) float64 8B 1.013e+03\n",
      "  * time     (time) datetime64[ns] 520B 2025-01-26T19:00:00 ... 2025-02-05T19...\n",
      "Data variables:\n",
      "    tp       (time, lev, lat, lon) float64 43MB 0.0 0.0 0.0 ... 0.8438 0.75\n",
      "Attributes:\n",
      "    title:        IFS Precipitation\n",
      "    conventions:  COARDS\n",
      "    datatype:     Grid\n",
      "    cachesize:    626240 bytes\n"
     ]
    }
   ],
   "source": [
    "xrain = prec\n",
    "print(np.array_equal(xrain, prec))\n",
    "\n",
    "for time in range (len(dates)):\n",
    "    for latitude in range(len(lat)) :\n",
    "        for longitude in range (len(lon)) :\n",
    "            if (time<=0):\n",
    "                if (xrain[time, latitude, longitude]<=0):\n",
    "                    xrain[time, latitude, longitude] == 0\n",
    "            elif(time>0):\n",
    "                if (xrain[time, latitude, longitude]<0):\n",
    "                    xrain[time, latitude, longitude] = xrain[time-1, latitude, longitude]\n",
    "                if (xrain[time, latitude, longitude]-xrain[time-1, latitude, longitude]<0) :\n",
    "                    xrain[time, latitude, longitude] = xrain[time-1, latitude, longitude]\n",
    "\n",
    "rain_interval = np.empty((len(dates),len(lat),len(lon)))\n",
    "\n",
    "rain_interval[0,:,:] = xrain[0,:,:]\n",
    "for i in range (1,len(dates)):\n",
    "    rain_interval[i,:,:] = xrain[i,:,:]-xrain[i-1,:,:]\n",
    "\n",
    "rain_four_demantion = rain_interval.reshape(len(dates), 1, len(lat), len(lon))\n",
    "\n",
    "rain_interval_result = xr.open_dataset(local_file_path)\n",
    "rain_interval_result['tp'].values = rain_four_demantion\n",
    "rain_interval_result = rain_interval_result.assign_coords(time=(\"time\",rain_interval_result['time'].values + np.timedelta64(7,'h')))\n",
    "\n",
    "output_rewrite = f\"ECMWF_new.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "output_path = os.path.join(path_nc_row, output_rewrite)\n",
    "rain_interval_result.to_netcdf(output_path)\n",
    "print (rain_interval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ECMWF_new_3d.0125.202501261200.PREC.nc berhasil dibuat\n",
      "{'lat': <class 'netCDF4.Variable'>\n",
      "float64 lat(lat)\n",
      "    grads_dim: y\n",
      "    grads_mapping: linear\n",
      "    grads_size: 185\n",
      "    units: degrees_north\n",
      "    long_name: latitude\n",
      "    minimum: -14.0\n",
      "    maximum: 9.0\n",
      "    resolution: -0.125\n",
      "unlimited dimensions: \n",
      "current shape = (185,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used, 'lon': <class 'netCDF4.Variable'>\n",
      "float64 lon(lon)\n",
      "    grads_dim: x\n",
      "    grads_mapping: linear\n",
      "    grads_size: 449\n",
      "    units: degrees_east\n",
      "    long_name: longitude\n",
      "    minimum: 92.0\n",
      "    maximum: 148.0\n",
      "    resolution: 0.125\n",
      "unlimited dimensions: \n",
      "current shape = (449,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used, 'lev': <class 'netCDF4.Variable'>\n",
      "float64 lev(lev)\n",
      "    grads_dim: z\n",
      "    grads_mapping: levels\n",
      "    units: millibar\n",
      "    long_name: altitude\n",
      "unlimited dimensions: \n",
      "current shape = (1,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used, 'time': <class 'netCDF4.Variable'>\n",
      "float64 time(time)\n",
      "    grads_dim: t\n",
      "    grads_mapping: linear\n",
      "    grads_size: 65\n",
      "    units: hours since 2025-01-26 12:00:00\n",
      "    grads_step: 3hr\n",
      "    long_name: validity time\n",
      "unlimited dimensions: \n",
      "current shape = (65,)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used, 'tp': <class 'netCDF4.Variable'>\n",
      "float64 tp(time, lev, lat, lon)\n",
      "    least_significant_digit: 3\n",
      "    longname: Precipitation Accumulation (mm)\n",
      "unlimited dimensions: \n",
      "current shape = (65, 1, 185, 449)\n",
      "filling on, default _FillValue of 9.969209968386869e+36 used}\n"
     ]
    }
   ],
   "source": [
    "result_file_name = f\"ECMWF_new_3d.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "\n",
    "# gabungkan path dengan nama file\n",
    "file_path = os.path.join(path_modified, result_file_name)\n",
    "\n",
    "# buat file NetCDF baru\n",
    "new_data_interval = Dataset(file_path, 'w', format='NETCDF4')\n",
    "print(f\"File {result_file_name} berhasil dibuat\")\n",
    "print(data.variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mengambil nilai untuk parameter rain, lat, long, time \n",
    "# rain diambil dari hasil interval rain_four_demantion\n",
    "# lat, lon dan time diambil dari data asli netcdf\n",
    "interval_for_rain = rain_interval[:48,:,:]\n",
    "interval_for_latitude = data.variables['lat'][:]\n",
    "interval_for_longitude = data.variables['lon'][:]\n",
    "interval_for_time = data.variables['time'][:48]\n",
    "\n",
    "# membuat demensi untuk netCDF\n",
    "new_data_interval.createDimension('lon', len(interval_for_longitude))\n",
    "new_data_interval.createDimension('lat', len(interval_for_latitude))\n",
    "new_data_interval.createDimension('time', len(interval_for_time))\n",
    "\n",
    "# membuat variabel untuk netCDF\n",
    "new_interval_for_longitude = new_data_interval.createVariable('lon', 'f4', 'lon')\n",
    "new_interval_for_latitude = new_data_interval.createVariable('lat', 'f4', 'lat')\n",
    "new_interval_for_rain = new_data_interval.createVariable('rain', 'f4', ('time', 'lat', 'lon'))\n",
    "new_interval_for_time = new_data_interval.createVariable('time', 'i4', 'time')\n",
    "\n",
    "# definisikan variabel dan simpan menjadi netCDF baru\n",
    "new_interval_for_longitude[:] = interval_for_longitude[:]\n",
    "new_interval_for_latitude[:] = interval_for_latitude[:]\n",
    "new_interval_for_rain[:, :, :] = rain_interval[:48,:,:]\n",
    "new_interval_for_time[:] = interval_for_time + 7\n",
    "\n",
    "# tambhin informasi umum\n",
    "new_data_interval.description = \"ECMWF from BMKG modified by Gerardus David\"\n",
    "new_data_interval.history = \"Cretaed\" + today.strftime(\"%d%m%y\")\n",
    "\n",
    "new_interval_for_longitude.units = \"degree_east\"\n",
    "new_interval_for_latitude.units = \"degree_north\"\n",
    "new_interval_for_time.units = \"hours since \"+(dates[0]+datetime.timedelta(hours=7)).strftime('%Y-%m-%d ')+str(cycle)+\":00:00\"\n",
    "new_interval_for_rain.units = \"mm\"\n",
    "\n",
    "new_data_interval.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lon': <class 'netCDF4.Variable'>\n",
       " float32 lon(lon)\n",
       "     units: degree_east\n",
       " unlimited dimensions: \n",
       " current shape = (449,)\n",
       " filling on, default _FillValue of 9.969209968386869e+36 used,\n",
       " 'lat': <class 'netCDF4.Variable'>\n",
       " float32 lat(lat)\n",
       "     units: degree_north\n",
       " unlimited dimensions: \n",
       " current shape = (185,)\n",
       " filling on, default _FillValue of 9.969209968386869e+36 used,\n",
       " 'rain': <class 'netCDF4.Variable'>\n",
       " float32 rain(time, lat, lon)\n",
       "     units: mm\n",
       " unlimited dimensions: \n",
       " current shape = (48, 185, 449)\n",
       " filling on, default _FillValue of 9.969209968386869e+36 used,\n",
       " 'time': <class 'netCDF4.Variable'>\n",
       " int32 time(time)\n",
       "     units: hours since 2025-01-26 12:00:00\n",
       " unlimited dimensions: \n",
       " current shape = (48,)\n",
       " filling on, default _FillValue of -2147483647 used}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_interval_data_path = f\"../repository/pre-processing/result-row\\ECMWF_new_3d.0125.{today.strftime('%Y%m%d')}{cycle}00.PREC.nc\"\n",
    "\n",
    "n = 0 \n",
    "\n",
    "interval_data = Dataset(new_interval_data_path)\n",
    "interval_data.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-28 01:00:00 WIB\n",
      "2025-01-28 19:00:00 WIB\n"
     ]
    }
   ],
   "source": [
    "interval_latitude = interval_data.variables[\"lat\"][:]\n",
    "interval_longitude = interval_data.variables[\"lon\"][:]\n",
    "interval_prec = interval_data.variables[\"rain\"][:,:,:]\n",
    "\n",
    "interval_time = interval_data.variables[\"time\"][:]\n",
    "interval_dates = num2date(interval_time, interval_data.variables[\"time\"].units)\n",
    "\n",
    "print((interval_dates[10+n]).strftime('%Y-%m-%d %H:%M:%S WIB'))\n",
    "print((interval_dates[16+n]).strftime('%Y-%m-%d %H:%M:%S WIB'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BACA DATA TABULAR UNTUK MEMBUAT DATA TABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max.columns\", None)\n",
    "grid = pd.read_excel(pch_tabular)\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46995"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Data index lon, lat, balai, ws, kota\n",
    "grid_long = grid['idx_long'].to_numpy()\n",
    "grid_lat = grid['idx_lat'].to_numpy()\n",
    "longitude_r = grid['long_data']\n",
    "latitude_r = grid['lat_data']\n",
    "latitude_prod = grid['lat_prod']\n",
    "longitude_prod = grid['long_prod']\n",
    "pulau = grid['pulau']\n",
    "balai= grid['balai']\n",
    "kode_balai = grid['kode_balai']\n",
    "ws = grid ['wilayah_sungai']\n",
    "das = grid['das']\n",
    "prov=grid[\"provinsi\"]\n",
    "kota = grid['kabkot']\n",
    "wilayah = grid['wilayah']\n",
    "latshape = grid_lat.shape[0]\n",
    "latshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025012721\n",
      "2025012800\n",
      "2025012803\n",
      "2025012806\n",
      "2025012809\n",
      "2025012812\n",
      "2025012815\n",
      "2025012818\n"
     ]
    }
   ],
   "source": [
    "# Forecasting 1 day ahead\n",
    "for k in range (11+n,19+n):\n",
    "    print((dates[k]).strftime(\"%Y%m%d%H\"))\n",
    "    idx_t=(dates[k]).strftime(\"%Y%m%d%H\")\n",
    "    if (k==19+n):\n",
    "        globals()['hujanharian_'+(idx_t)] = interval_prec[11+n,:,:]\n",
    "    else:\n",
    "        globals()['hujanharian_'+(idx_t)] = interval_prec[11+n:k+1,:,:].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025012821\n",
      "2025012900\n",
      "2025012903\n",
      "2025012906\n",
      "2025012909\n",
      "2025012912\n",
      "2025012915\n",
      "2025012918\n"
     ]
    }
   ],
   "source": [
    "# Forecasting 2 days ahead\n",
    "for k in range (11+8+n,19+8+n):\n",
    "    print((dates[k]).strftime(\"%Y%m%d%H\"))\n",
    "    idx_t = (dates[k]).strftime(\"%Y%m%d%H\")\n",
    "    if (k == 19 + 8 + n):\n",
    "        globals()['hujanharian_'+(idx_t)] = prec[11+8+n,:,:]\n",
    "    else:\n",
    "        globals()['hujanharian_'+(idx_t)] = prec[11+8+n:k+1,:,:].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kolom = ['long_prod', 'lat_prod', 'tanggal', 'longitude','latitude','pulau', 'kode_balai', 'balai','das','provinsi','kabkot','wilayah']\n",
    "\n",
    "df_waspada = pd.DataFrame(columns=kolom)\n",
    "\n",
    "for tab in range (latshape) :\n",
    "    gridlat = grid_lat[tab]\n",
    "    gridlon = grid_long[tab]\n",
    "    for k in range (11 + n, 27 + n):\n",
    "        #utk cek awal mulai waspada\n",
    "        idx_t = (dates[k]).strftime(\"%Y%m%d%H\")\n",
    "        idx_h = (dates[k]).strftime(\"%H:00\")\n",
    "        hujan_cek = globals()['hujanharian_'+(idx_t)]\n",
    "        \n",
    "        #untuk tanggal status siaga banjir dan pengecekan status akhir siaga banjir di tiap grid\n",
    "        i_idx = 11 + n if k < 19 + n else 19 + n\n",
    "        tanggal = (dates[i_idx]).strftime(\"%d %B %Y\")\n",
    "        \n",
    "        if (hujan_cek[gridlat, gridlon] >= 0.5):\n",
    "            df = pd.DataFrame([{'tanggal':tanggal, 'long_prod':longitude_prod[tab], 'lat_prod':latitude_prod[tab], 'longitude':longitude_r[tab],'latitude':latitude_r[tab], 'pulau':pulau[tab], 'kode_balai':kode_balai[tab], 'balai':balai[tab],\\\n",
    "                             'das':das[tab],'provinsi':prov[tab],'kabkot':kota[tab],'wilayah':wilayah[tab]\\\n",
    "                              ,'waktu_mulai':idx_h}])\n",
    "            i_idx = (11+n) if k < (19+n) else (19+n)\n",
    "            for i in range (i_idx, i_idx + 8):\n",
    "                idx_t = (dates[i]).strftime(\"%Y%m%d%H\")\n",
    "                idx_h = (dates[i]).strftime(\"%H:00\")\n",
    "                df['ch_' + idx_h] = globals()['hujanharian_'+(idx_t)][gridlat, gridlon]\n",
    "            \n",
    "            kelas=globals()['hujanharian_'+(idx_t)][gridlat, gridlon]\n",
    "            if (0.5 < kelas <= 20):\n",
    "                status=\"1\" #HUJAN RINGAN\n",
    "            elif(20 < kelas <= 50):\n",
    "                status=\"2\" #HUJAN SEDANG\n",
    "            elif(50 < kelas <=100):\n",
    "                status=\"3\" #HUJAN LEBAT\n",
    "            elif(100 < kelas <= 150):\n",
    "                status=\"4\" #HUJAN SANGAT LEBAT\n",
    "            elif(kelas > 150):\n",
    "                status=\"5\" #HUJAN EKSTREM\n",
    "                \n",
    "            df[\"klasifikasi_hujan\"] = status\n",
    "            \n",
    "            status_cek = globals()['hujanharian_'+(idx_t)][gridlat, gridlon]\n",
    "            if (0.5 < status_cek <= 50):\n",
    "                status_1=\"1\" #AMAN\n",
    "            elif(50 < status_cek <= 75):\n",
    "                status_1=\"2\" #WASPADA\n",
    "            elif(75 < status_cek <= 100):\n",
    "                status_1=\"3\" #SIAGA\n",
    "            elif(status_cek > 100):\n",
    "                status_1=\"4\" #AWAS\n",
    "            \n",
    "            df[\"status_akhir\"] = status_1\n",
    "            \n",
    "            df_waspada = pd.concat([df_waspada, df])\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "df = df_waspada.sort_values(by=\"tanggal\")\n",
    "df = df.set_index(\"tanggal\")\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('../repository/pre-processing/accumulation/accum_'+\\\n",
    "                        (dates[11+n]).strftime('%m%d%Y')+'_' #%Y%m%d\n",
    "                        +(dates[26+n]).strftime('%m%d%Y')+'.xlsx', engine='xlsxwriter')\n",
    "# Write each dataframe to a different worksheet.\n",
    "df.to_excel(writer, sheet_name='Akumulasi Berjalan')\n",
    "writer.close()\n",
    "\n",
    "print('Done..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_excel(\"../new-repository/pre-processing/accumulation/accum_01172025_01192025.xlsx\")\n",
    "data = pd.read_excel(\n",
    "    f\"../repository/pre-processing/accumulation/accum_{(dates[11+n]).strftime('%m%d%Y')}_{(dates[26+n]).strftime('%m%d%Y')}.xlsx\"\n",
    ")\n",
    "\n",
    "# data['tanggal'] = pd.to_timedelta(data['tanggal'])\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "tanggal_hari_ini = datetime.now()\n",
    "tanggal_besok = tanggal_hari_ini + timedelta(days=1)\n",
    "\n",
    "# Formatkan tanggal besok ke string\n",
    "tanggal_besok_str = tanggal_besok.strftime(\"%d %B %Y\")\n",
    "print(\"Tanggal besok:\", tanggal_besok_str)\n",
    "\n",
    "data_tanggal_besok = data[data['tanggal'] == tanggal_besok_str]\n",
    "data_tanggal_besok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tanggal_besok_sorted = data_tanggal_besok.sort_values(by='ch_01:00', ascending=False)\n",
    "data_tanggal_besok_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
